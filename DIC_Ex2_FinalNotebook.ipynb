{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ad1ab8-bebd-4443-b08f-64fb96b683dd",
   "metadata": {
    "id": "b7ad1ab8-bebd-4443-b08f-64fb96b683dd",
    "outputId": "0627400f-a397-4691-a755-630351a63568",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "sc = SparkContext(appName=\"ChiSquare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee3d00-ec3b-4d4f-9995-8baaf74c9c31",
   "metadata": {
    "id": "f1ee3d00-ec3b-4d4f-9995-8baaf74c9c31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    STOPWORDS = set(line.strip() for line in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e706c16-a24d-4210-8ce0-30b9ede72577",
   "metadata": {
    "id": "9e706c16-a24d-4210-8ce0-30b9ede72577"
   },
   "source": [
    "# Part 1) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef144a2-36a1-4fe5-be61-cdade66424fc",
   "metadata": {
    "id": "eef144a2-36a1-4fe5-be61-cdade66424fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_category(line):\n",
    "    \"\"\"\n",
    "    The function parses the input line and extracts the category of the review.\n",
    "    Input:\n",
    "        line: json representing the review\n",
    "    Output:\n",
    "        category\n",
    "    \"\"\"\n",
    "    review = json.loads(line)\n",
    "    return review[\"category\"]\n",
    "\n",
    "def map_words(line, stopwords):\n",
    "    \"\"\"\n",
    "    The function parses the input line and extracts the category and text from the review.\n",
    "    Then it carries out the tokenization, case folding and stopword filtering on the review text.\n",
    "    Finally, it yields a tuple per every unique word extracted, holding the information regarding the category\n",
    "    Input:\n",
    "        line: line from the text file containing a json representing a review\n",
    "        stopwords\n",
    "    Output:\n",
    "        key: word\n",
    "        value: (category, 1)\n",
    "    \"\"\"\n",
    "    regex = re.compile(r\"[\\s\\t\\d\\[\\]\\{\\}().!?,;:+=\\-_\\\"'`~#@&*%€$§\\\\/]+\")\n",
    "    review = json.loads(line)\n",
    "    cat = review[\"category\"]\n",
    "    word_list = map(lambda word: word.lower(), regex.split(review[\"reviewText\"]))\n",
    "    unique_words = {word for word in word_list if word and word not in stopwords}\n",
    "    return [(word, (cat, 1)) for word in unique_words]\n",
    "\n",
    "def reducer_words(values):\n",
    "   \"\"\"\n",
    "    Aggregates word counts per category and calculates the total occurrences of each word.\n",
    "    Input:\n",
    "        values: a list of tuples where each tuple contains a category and a count (e.g., (category, 1))\n",
    "    Output:\n",
    "        A list of tuples, each containing:\n",
    "            - cat: category\n",
    "            - all_occur: the total occurrences of the word across all categories\n",
    "            - c: the count of the word within that category\n",
    "    \"\"\"\n",
    "    all_occur = 0\n",
    "    cat_counts = {}\n",
    "    for cat, c in values:\n",
    "        all_occur += c\n",
    "        cat_counts[cat] = cat_counts.get(cat, 0) + c\n",
    "    return [(cat, all_occur, c) for cat, c in cat_counts.items()]\n",
    "\n",
    "def calculate_chi_squared(n, cat_count, all_occur, a):\n",
    "    \"\"\"\n",
    "    Calculates the chi_squared score for category and word based on the provided metrics.\n",
    "    Input:\n",
    "        n: total number of reviews\n",
    "        cat_count: total number of reviews in category\n",
    "        all_occur: total number of occurrences of word\n",
    "        a: number of reviews in given category which contain the given word\n",
    "    Return:\n",
    "        -1 if the calculation encounters division by zero\n",
    "        float chi_squared score otherwise\n",
    "    \"\"\"\n",
    "    c = cat_count - a\n",
    "    b = all_occur - a\n",
    "    d = n - cat_count - b\n",
    "    denom = (a + b) * (a + c) * (b + d) * (c + d)\n",
    "    if denom == 0:\n",
    "        return -1\n",
    "    return n * (a * d - b * c) ** 2 / denom\n",
    "\n",
    "def reducer_scores(values, n, cat_count):\n",
    "    \"\"\"\n",
    "    Reduces intermediate word data to Chi-Square scores and selects the top 75 terms per category.\n",
    "    Input:\n",
    "        values: a list of tuples containing word data (word, all_occurrences, count in category)\n",
    "        n: total number of reviews\n",
    "        cat_count: total number of reviews in the category\n",
    "    Output:\n",
    "        A sorted list of tuples containing Chi-Square scores and words, limited to the top 75.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for word, all_occur, a in values:\n",
    "        score = calculate_chi_squared(n, cat_count, all_occur, a)\n",
    "        scores.append((score, word))\n",
    "    return sorted(scores, reverse=True)[:75]\n",
    "\n",
    "def run_chi_squared(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Executes the Chi-Square calculation on the input data, using the helper functions defined above, and writes the results to the output file.\n",
    "    Input:\n",
    "        input_file: path to the input file containing review data\n",
    "        output_file: path to the output file where results will be written\n",
    "    \"\"\"\n",
    "    stopwords_broad = sc.broadcast(STOPWORDS)\n",
    "\n",
    "    reviews = sc.textFile(input_file)\n",
    "\n",
    "    cat_counts = reviews.map(map_category).countByValue()\n",
    "    n = sum(cat_counts.values())\n",
    "\n",
    "    first_step = reviews.flatMap(lambda x: map_words(x, stopwords_broad.value)).groupByKey().flatMapValues(reducer_words).map(lambda x: (x[1][0], (x[0], x[1][1], x[1][2]))).cache()\n",
    "    second_step = first_step.groupByKey().map(lambda x: (x[0], reducer_scores(x[1], n, cat_counts[x[0]]))).cache()\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for category, scores in sorted(second_step.collect()):\n",
    "            chi_sq = [f\"{word}:{score}\" for score, word in scores]\n",
    "            f.write(f\"<{category}> {' '.join(chi_sq)}\\n\")\n",
    "\n",
    "        all_words = sorted(set(second_step.flatMap(lambda x: x[1]).map(lambda x: x[1]).collect()))\n",
    "        f.write(f\"{' '.join(all_words)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c794c595-5f4a-4fd1-a140-c26d882109b2",
   "metadata": {
    "id": "c794c595-5f4a-4fd1-a140-c26d882109b2",
    "outputId": "83ee6be2-a384-4ec2-a87d-37eddc925a3f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 47.8 ms, total: 227 ms\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_chi_squared(\"reviews_devset.json\",\"output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019062c-b8e5-464f-a3a5-57a7381e166f",
   "metadata": {
    "id": "6019062c-b8e5-464f-a3a5-57a7381e166f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "run_chi_squared(\"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\",\"output_rdd_fulldata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a123229-7b9f-4e09-93b2-9d689bc19e4c",
   "metadata": {
    "id": "8a123229-7b9f-4e09-93b2-9d689bc19e4c"
   },
   "source": [
    "# Part 2) Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c986f5d-09b1-4fc3-878c-ffd130db734c",
   "metadata": {
    "id": "4c986f5d-09b1-4fc3-878c-ffd130db734c"
   },
   "source": [
    "## DataFrame Creation\n",
    "\n",
    "In this section we create the dataframe from the reviews file, which will contain two columns `reviewText` and `category` extracted from the  review JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2609eeac-076c-41d2-8041-d44884c1f886",
   "metadata": {
    "id": "2609eeac-076c-41d2-8041-d44884c1f886",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE = \"reviews_devset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa45a16-a96c-45ce-8f7a-456bff3a4d50",
   "metadata": {
    "id": "7aa45a16-a96c-45ce-8f7a-456bff3a4d50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TextClassification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5412a1-10ba-4bb4-a3f3-58c6038c4275",
   "metadata": {
    "id": "0c5412a1-10ba-4bb4-a3f3-58c6038c4275",
    "outputId": "7bd33992-82e2-41f6-bce7-4ee128fcf477",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(FILE)\n",
    "df = reviews.select(\"reviewText\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ffab45-f408-4a3a-924e-b888b4c4b963",
   "metadata": {
    "id": "95ffab45-f408-4a3a-924e-b888b4c4b963",
    "outputId": "f2ef7e9a-f781-4426-cdda-5c7d59790171",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d1bcfb-1908-431d-85da-8486675f2c92",
   "metadata": {
    "id": "68d1bcfb-1908-431d-85da-8486675f2c92",
    "outputId": "489e466e-4337-499f-c8aa-4999b7546fec",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\", category='Patio_Lawn_and_Garde')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a87f0e-77e5-444e-b348-c97542e17051",
   "metadata": {
    "id": "b7a87f0e-77e5-444e-b348-c97542e17051"
   },
   "source": [
    "## ML Pipeline\n",
    "\n",
    "In the following section we set up the transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64086786-5785-486e-8cd4-7fecc1e9173e",
   "metadata": {
    "id": "64086786-5785-486e-8cd4-7fecc1e9173e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, ChiSqSelector, StringIndexer, CountVectorizer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28cfede-8f97-41b1-93f1-81699d7ca1b1",
   "metadata": {
    "id": "c28cfede-8f97-41b1-93f1-81699d7ca1b1",
    "outputId": "abb5b931-7cdb-45c5-d483-4207a7cbf1c2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:23:17 WARN DAGScheduler: Broadcasting large task binary with size 1074.0 KiB\n",
      "24/05/26 15:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1076.1 KiB\n",
      "24/05/26 15:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1078.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"allWords\", pattern=r\"[\\s\\t\\d\\[\\]\\{\\}().!?,;:+=\\-_\\\"'`~#@&*%€$§\\\\/]+\") # converts to lowercase and then tokenizes\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=list(STOPWORDS))\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") # category has to be numeric\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=cv.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, indexer, selector])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4fc3bc-3953-482a-b282-e3ff0f288981",
   "metadata": {
    "id": "0c4fc3bc-3953-482a-b282-e3ff0f288981",
    "outputId": "e8cc80a3-5c76-4dab-d914-3a805ed535a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:24:10 WARN DAGScheduler: Broadcasting large task binary with size 1078.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|          reviewText|            category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|This was a gift f...|Patio_Lawn_and_Garde|[this, was, a, gi...|[gift, husband, m...|(96130,[2,3,7,8,3...|         18.0|(2000,[2,3,7,8,35...|\n",
      "|This is a very ni...|Patio_Lawn_and_Garde|[this, is, a, ver...|[nice, spreader, ...|(96130,[0,1,3,21,...|         18.0|(2000,[0,1,3,21,3...|\n",
      "|The metal base wi...|Patio_Lawn_and_Garde|[the, metal, base...|[metal, base, hos...|(96130,[4,10,29,1...|         18.0|(2000,[4,10,174,3...|\n",
      "|For the most part...|Patio_Lawn_and_Garde|[for, the, most, ...|[part, works, pre...|(96130,[1,3,4,9,1...|         18.0|(2000,[1,3,4,9,10...|\n",
      "|This hose is supp...|Patio_Lawn_and_Garde|[this, hose, is, ...|[hose, supposed, ...|(96130,[12,32,42,...|         18.0|(2000,[12,29,101,...|\n",
      "|This tool works v...|Patio_Lawn_and_Garde|[this, tool, work...|[tool, works, cut...|(96130,[0,3,4,8,1...|         18.0|(2000,[0,3,4,8,11...|\n",
      "|This product is a...|Patio_Lawn_and_Garde|[this, product, i...|[typical, usable,...|(96130,[18,63,122...|         18.0|(2000,[18,112,175...|\n",
      "|I was excited to ...|Patio_Lawn_and_Garde|[i, was, excited,...|[excited, ditch, ...|(96130,[6,21,35,3...|         18.0|(2000,[6,21,32,36...|\n",
      "|I purchased the L...|Patio_Lawn_and_Garde|[i, purchased, th...|[purchased, leaf,...|(96130,[3,4,5,6,4...|         18.0|(2000,[3,4,5,6,40...|\n",
      "|Never used a manu...|Patio_Lawn_and_Garde|[never, used, a, ...|[manual, lawnmowe...|(96130,[6,8,41,87...|         18.0|(2000,[6,8,38,78,...|\n",
      "|Good price. Good ...|Patio_Lawn_and_Garde|[good, price, goo...|[good, price, goo...|(96130,[1,13,95,2...|         18.0|(2000,[1,13,226],...|\n",
      "|I have owned the ...|Patio_Lawn_and_Garde|[i, have, owned, ...|[owned, flowtron,...|(96130,[5,17,36,4...|         18.0|(2000,[5,17,33,40...|\n",
      "|I had \"won\" a sim...|Patio_Lawn_and_Garde|[i, had, won, a, ...|[similar, family,...|(96130,[1,11,31,3...|         18.0|(2000,[1,11,28,35...|\n",
      "|The birds ate all...|Patio_Lawn_and_Garde|[the, birds, ate,...|[birds, ate, blue...|(96130,[44,160,28...|         18.0|(2000,[40,144,339...|\n",
      "|Bought last summe...|Patio_Lawn_and_Garde|[bought, last, su...|[bought, summer, ...|(96130,[0,3,7,9,1...|         18.0|(2000,[0,3,7,9,11...|\n",
      "|I knew I had a mo...|Patio_Lawn_and_Garde|[i, knew, i, had,...|[knew, mouse, bas...|(96130,[8,28,29,6...|         18.0|(2000,[8,26,57,80...|\n",
      "|I was a little wo...|Patio_Lawn_and_Garde|[i, was, a, littl...|[worried, reading...|(96130,[1,15,130,...|         18.0|(2000,[1,15,120,1...|\n",
      "|I have used this ...|Patio_Lawn_and_Garde|[i, have, used, t...|[brand, long, tim...|(96130,[2,3,23,25...|         18.0|(2000,[2,3,221,26...|\n",
      "|I actually do not...|Patio_Lawn_and_Garde|[i, actually, do,...|[current, model, ...|(96130,[4,10,16,2...|         18.0|(2000,[4,10,16,20...|\n",
      "|Just what I  expe...|Patio_Lawn_and_Garde|[just, what, i, e...|[expected, works,...|(96130,[0,18,33,4...|         18.0|(2000,[0,18,30,42...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09c9c5-087b-4259-afb3-1a34ee62f165",
   "metadata": {
    "id": "bc09c9c5-087b-4259-afb3-1a34ee62f165"
   },
   "source": [
    "## Top 2000 Terms\n",
    "\n",
    "By fitting the pipeline on all the data we can extract the top 2000 terms according to the chi-squared feature selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8448cc-06f6-484d-a751-71a0bd8e834b",
   "metadata": {
    "id": "2b8448cc-06f6-484d-a751-71a0bd8e834b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_features = model.stages[-1].selectedFeatures\n",
    "vocabulary = model.stages[-3].vocabulary[:(max(top_features) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f9dc81-aaf1-48d0-a5ff-64bb08d2039f",
   "metadata": {
    "id": "b1f9dc81-aaf1-48d0-a5ff-64bb08d2039f"
   },
   "outputs": [],
   "source": [
    "\n",
    "top_terms_model = [vocabulary[i] for i in top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61bf3958-2b63-4bf1-a620-4f63fda5b59d",
   "metadata": {
    "id": "61bf3958-2b63-4bf1-a620-4f63fda5b59d",
    "outputId": "deae36cc-0310-4418-e9df-f0a0ec3367ec",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'good',\n",
       " 'love',\n",
       " 'time',\n",
       " 'work',\n",
       " 'recommend',\n",
       " 'back',\n",
       " 'easy',\n",
       " 'make',\n",
       " 'bought',\n",
       " 'made',\n",
       " 'find',\n",
       " 'buy',\n",
       " 'price',\n",
       " 'put',\n",
       " 'reading',\n",
       " 'quality',\n",
       " 'people',\n",
       " 'works',\n",
       " 'quot']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms_model[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a199f035-e399-407f-a7de-5e18935c5f54",
   "metadata": {
    "id": "a199f035-e399-407f-a7de-5e18935c5f54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(top_terms_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8214bb-2615-4190-aed2-41fbf1ec03af",
   "metadata": {
    "id": "bc8214bb-2615-4190-aed2-41fbf1ec03af",
    "tags": []
   },
   "source": [
    "## Comparison With Assignement 1\n",
    "\n",
    "We performed the chi-square calculation in the first assignement to calculate the top 75 terms in each review category. We now want ot compare these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52f45-2ce0-4b50-bcd7-f185e23c393e",
   "metadata": {
    "id": "abd52f45-2ce0-4b50-bcd7-f185e23c393e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_terms_1 = []\n",
    "with open(\"output_1.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    top_terms_1 = lines[-1].strip().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64c3e7-704c-469d-99ad-8b3e8179671f",
   "metadata": {
    "id": "9b64c3e7-704c-469d-99ad-8b3e8179671f",
    "outputId": "38969b04-3a16-46f6-8d87-d953585e40dd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_terms_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694b3c4-1ea0-40ff-97ea-342dcaa7ec9d",
   "metadata": {
    "id": "a694b3c4-1ea0-40ff-97ea-342dcaa7ec9d"
   },
   "source": [
    "In the first assignemnt we ended up with 1464 terms in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab194d6-ea03-42ee-90e4-5117b52a44f6",
   "metadata": {
    "id": "eab194d6-ea03-42ee-90e4-5117b52a44f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra = [word for word in top_terms_model if word not in top_terms_1]\n",
    "missing = [word for word in top_terms_1 if word not in top_terms_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fa0e7-b2fe-4d12-a75c-a33367651eb8",
   "metadata": {
    "id": "a51fa0e7-b2fe-4d12-a75c-a33367651eb8",
    "outputId": "f4c7b2ec-af55-44b9-a331-cce8c03eb48c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e14b2c-17be-4266-8b93-acf88e35f695",
   "metadata": {
    "id": "84e14b2c-17be-4266-8b93-acf88e35f695",
    "outputId": "b7021cc0-9a68-428d-b52f-e284e936445d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de291cd-1001-4578-86aa-651e7e92bb4e",
   "metadata": {
    "id": "2afc7781-c75c-4cd4-bb75-bc295e0165b9"
   },
   "source": [
    "Based solely on the number of extra words, and the number of missing words we can conclud that these two results differ greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7608b-ee7e-4dae-a2c9-fa0bdbb1215b",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8fd5c-48b0-4133-b1ec-43257fdad260",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91e5722-4b4f-493f-9d0b-528e4159cbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, ChiSqSelector, Normalizer\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "import difflib\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TextClassification_Pt2\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"reviews_devset.json\").select(\"reviewText\", \"category\")\n",
    "df.printSchema()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TextClassification\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584e653-cc49-4a05-960d-565307137e49",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cc1072-c3e0-4be7-a21b-73e736b2593e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training (70%) and test (30%) sets\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14ef7e-c950-4142-ab83-b85d1ee786a4",
   "metadata": {},
   "source": [
    "## Extend the pipeline and Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd636cc5-da37-4ee6-8e7e-2225b2d79326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"allWords\", pattern=r\"[\\s\\t\\d\\[\\]\\{\\}().!?,;:+=\\-_\\\"'`~#@&*%€$§\\\\/]+\") # converts to lowercase and then tokenizes\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\")\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") # category has to be numeric\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=cv.getOutputCol(), outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "normalizer = Normalizer(inputCol=selector.getOutputCol(), outputCol=\"normFeatures\")\n",
    "svc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovr = OneVsRest(classifier=svc, featuresCol=\"normFeatures\", labelCol=\"categoryIndex\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, indexer, selector, normalizer, ovr])\n",
    "\n",
    "# Fit the model\n",
    "# model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c4fe3-ee9e-4b91-a892-59ec2d927f22",
   "metadata": {},
   "source": [
    "### Grid Search for Parameter Optimization¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0cea867-840b-417e-94d7-d022f088c17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1, 0.6]) \\\n",
    "    .addGrid(svc.maxIter, [3, 5]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afac5f-e256-4f16-81ba-673b34819024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5d3c8-aa1a-4afc-a12d-4dc862043ba3",
   "metadata": {},
   "source": [
    "### Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2a86e-5c7e-4b69-a716-81b9848d1871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 16:57:43 WARN TaskSetManager: Stage 6832 contains a task of very large size (1487 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model1.bestModel.write().overwrite().save(\"model_devset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e38e5-6475-48ed-bd8b-0cfdfd52e472",
   "metadata": {},
   "source": [
    "### Load the saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c68ab82-07b3-4a8f-9041-dda70b2c7f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"model_devset\"\n",
    "loaded_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb49ceb-42c4-4d8b-948c-e8605ff03e79",
   "metadata": {},
   "source": [
    "### Transform train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e158fcd2-f35a-43bc-b1d4-e2b063961539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|          category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|       rawPrediction|prediction|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|                    |              Book|                  []|                  []|       (80126,[],[])|          0.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |              Book|                  []|                  []|       (80126,[],[])|          0.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |     CDs_and_Vinyl|                  []|                  []|       (80126,[],[])|          5.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |Sports_and_Outdoor|                  []|                  []|       (80126,[],[])|          7.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |Sports_and_Outdoor|                  []|                  []|       (80126,[],[])|          7.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |     Toys_and_Game|                  []|                  []|       (80126,[],[])|         11.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "| This is a book t...|              Book|[this, is, a, boo...|[book, difficult,...|(80126,[0,1,3,5,1...|          0.0|(2000,[0,1,3,5,10...|(2000,[0,1,3,5,10...|[1.91762831897961...|       0.0|\n",
      "|\" 'Criticizing an...|              Book|[criticizing, and...|[criticizing, cen...|(80126,[0,1,2,6,1...|          0.0|(2000,[0,1,2,6,11...|(2000,[0,1,2,6,11...|[3.31529958304274...|       0.0|\n",
      "|\".....INDIANA PAC...|              Book|[indiana, pacers,...|[indiana, pacers,...|(80126,[0,1,49,53...|          0.0|(2000,[0,1,45,49,...|(2000,[0,1,45,49,...|[2.55475865776195...|       0.0|\n",
      "|\"A River Runs Thr...|              Book|[a, river, runs, ...|[river, runs, rem...|(80126,[1,2,4,12,...|          0.0|(2000,[1,2,4,11,1...|(2000,[1,2,4,11,1...|[3.79527585010808...|       0.0|\n",
      "|\"An unforgettable...|              Book|[an, unforgettabl...|[unforgettable, s...|(80126,[12,35,62,...|          0.0|(2000,[11,32,56,6...|(2000,[11,32,56,6...|[2.28806905259109...|       0.0|\n",
      "|\"Angels\" is a lov...|              Book|[angels, is, a, l...|[angels, lovely, ...|(80126,[0,1,5,9,1...|          0.0|(2000,[0,1,5,8,16...|(2000,[0,1,5,8,16...|[3.68426103274754...|       0.0|\n",
      "|\"Bad Intentions\" ...|              Book|[bad, intentions,...|[bad, intentions,...|(80126,[0,5,11,15...|          0.0|(2000,[0,5,10,14,...|(2000,[0,5,10,14,...|[4.16316039018823...|       0.0|\n",
      "|\"Bold Fresh\" is a...|              Book|[bold, fresh, is,...|[bold, fresh, qui...|(80126,[0,1,5,6,8...|          0.0|(2000,[0,1,5,6,10...|(2000,[0,1,5,6,10...|[5.81422245954172...|       0.0|\n",
      "|\"Canada\" is the f...|              Book|[canada, is, the,...|[canada, first, r...|(80126,[0,5,12,15...|          0.0|(2000,[0,5,11,14,...|(2000,[0,5,11,14,...|[4.74606906444269...|       0.0|\n",
      "|\"Cathedral\" was a...|              Book|[cathedral, was, ...|[cathedral, actua...|(80126,[1,2,5,9,1...|          0.0|(2000,[1,2,5,8,10...|(2000,[1,2,5,8,10...|[0.75938459335958...|       0.0|\n",
      "|\"Ceremonies for R...|              Book|[ceremonies, for,...|[ceremonies, real...|(80126,[0,2,5,16,...|          0.0|(2000,[0,2,5,15,1...|(2000,[0,2,5,15,1...|[1.84256766631502...|       0.0|\n",
      "|\"Dead Men Don't L...|              Book|[dead, men, don, ...|[dead, men, leave...|(80126,[0,1,3,9,1...|          0.0|(2000,[0,1,3,8,10...|(2000,[0,1,3,8,10...|[0.76495110586141...|       0.0|\n",
      "|\"Does the United ...|              Book|[does, the, unite...|[united, states, ...|(80126,[0,1,11,19...|          0.0|(2000,[0,1,10,65,...|(2000,[0,1,10,65,...|[1.05772673157740...|       0.0|\n",
      "|\"Enemy of the Fae...|              Book|[enemy, of, the, ...|[enemy, fae, stan...|(80126,[0,1,3,6,9...|          0.0|(2000,[0,1,3,6,8,...|(2000,[0,1,3,6,8,...|[3.65744548402523...|       0.0|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5053a1f0-bbc5-4930-b039-a5e971b19a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = loaded_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a681f190-3baf-4053-b45d-c147484763ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|          category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|       rawPrediction|prediction|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|                    |              Book|                  []|                  []|       (80126,[],[])|          0.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |Sports_and_Outdoor|                  []|                  []|       (80126,[],[])|          7.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |     Toys_and_Game|                  []|                  []|       (80126,[],[])|         11.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|                    |     Toys_and_Game|                  []|                  []|       (80126,[],[])|         11.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|\"101 Money Saving...|              Book|[money, saving, t...|[money, saving, t...|(80126,[0,17,20,2...|          0.0|(2000,[0,16,18,19...|(2000,[0,16,18,19...|[1.24003379764166...|       0.0|\n",
      "|\"14 Formulas for ...|              Book|[formulas, for, p...|[formulas, painti...|(80126,[0,15,21,2...|          0.0|(2000,[0,14,19,23...|(2000,[0,14,19,23...|[7.28233864751578...|       0.0|\n",
      "|\"A Holiday Explai...|              Book|[a, holiday, expl...|[holiday, explain...|(80126,[0,1,3,5,8...|          0.0|(2000,[0,1,3,5,11...|(2000,[0,1,3,5,11...|[2.83972270187922...|       0.0|\n",
      "|\"Babyz Sasha\" is ...|     Toys_and_Game|[babyz, sasha, is...|[babyz, sasha, su...|(80126,[1,3,6,10,...|         11.0|(2000,[1,3,6,9,13...|(2000,[1,3,6,9,13...|[-4.0149649566098...|      11.0|\n",
      "|\"Bedtime Prayers\"...|              Book|[bedtime, prayers...|[bedtime, prayers...|(80126,[0,3,4,5,8...|          0.0|(2000,[0,3,4,5,11...|(2000,[0,3,4,5,11...|[3.68656234999946...|       0.0|\n",
      "|\"Bouncing Back\" i...|              Book|[bouncing, back, ...|[bouncing, back, ...|(80126,[0,2,6,24,...|          0.0|(2000,[0,2,6,22,2...|(2000,[0,2,6,22,2...|[6.02877223763945...|       0.0|\n",
      "|\"But I Just Grew ...|              Book|[but, i, just, gr...|[grew, bangs, inc...|(80126,[0,1,6,7,1...|          0.0|(2000,[0,1,6,7,11...|(2000,[0,1,6,7,11...|[1.25767757354391...|       0.0|\n",
      "|\"Chase\" is a wome...|              Book|[chase, is, a, wo...|[chase, women, bi...|(80126,[0,5,9,10,...|          0.0|(2000,[0,5,8,9,10...|(2000,[0,5,8,9,10...|[3.30274557829887...|       0.0|\n",
      "|\"Cities of the Pl...|              Book|[cities, of, the,...|[cities, plain, e...|(80126,[0,4,5,6,3...|          0.0|(2000,[0,4,5,6,27...|(2000,[0,4,5,6,27...|[3.04754267379362...|       0.0|\n",
      "|\"Dark City\" is a ...|              Book|[dark, city, is, ...|[dark, city, jour...|(80126,[0,7,9,11,...|          0.0|(2000,[0,7,8,10,1...|(2000,[0,7,8,10,1...|[1.43746474587411...|       3.0|\n",
      "|\"Digging Deep\" en...|              Book|[digging, deep, e...|[digging, deep, e...|(80126,[0,2,5,8,1...|          0.0|(2000,[0,2,5,11,1...|(2000,[0,2,5,11,1...|[3.84092626859801...|       0.0|\n",
      "|\"Dragonfly Kisses...|              Book|[dragonfly, kisse...|[dragonfly, kisse...|(80126,[1,4,5,6,1...|          0.0|(2000,[1,4,5,6,9,...|(2000,[1,4,5,6,9,...|[2.78983344236473...|       0.0|\n",
      "|\"Empire of the Su...|              Book|[empire, of, the,...|[empire, summer, ...|(80126,[0,5,9,12,...|          0.0|(2000,[0,5,8,11,1...|(2000,[0,5,8,11,1...|[4.65378281426285...|       0.0|\n",
      "|\"Excelente\". exce...|        Automotive|[excelente, excel...|[excelente, excel...|(80126,[123,704,1...|         14.0|(2000,[96,1613],[...|(2000,[96,1613],[...|[-0.4480758828301...|       0.0|\n",
      "|\"First Church of ...|              Book|[first, church, o...|[first, church, e...|(80126,[0,5,12,15...|          0.0|(2000,[0,5,11,14,...|(2000,[0,5,11,14,...|[3.89562727860076...|       0.0|\n",
      "|\"First Comes Marr...|              Book|[first, comes, ma...|[first, comes, ma...|(80126,[1,9,15,17...|          0.0|(2000,[1,8,14,16,...|(2000,[1,8,14,16,...|[0.98600440287580...|       0.0|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643e24a-fd65-4d94-9666-76149e7c0162",
   "metadata": {},
   "source": [
    "### Create PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ddd2dd-58f7-4050-9eed-6d18e2b3f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=100, inputCol= selector.getOutputCol(), outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200689d6-4e28-4b94-b82e-f628e78e1496",
   "metadata": {},
   "source": [
    "### Extend the pipeline using pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ecd8220-ab7f-4f9e-8a12-edbb5f7fa7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, indexer, selector, normalizer, pca, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5979c3-f6c7-4264-82a0-76f8a7bc2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:34:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/05/26 15:36:28 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pca_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ea59d-ec81-4a5e-99b8-143719879df7",
   "metadata": {},
   "source": [
    "### Save the pca model and transform train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a614fcf-c1e4-4df8-909c-fcf338587bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:44:11 WARN TaskSetManager: Stage 1304 contains a task of very large size (1504 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/26 15:44:26 WARN TaskSetManager: Stage 1317 contains a task of very large size (1603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pca_model.write().overwrite().save(\"pca_model_devset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f967ce-cf17-4127-8ae1-b95c0282090d",
   "metadata": {},
   "source": [
    "Load the saved pca model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "febee371-ce23-4b0d-93c8-43820bdc2f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_path = \"pca_model_devset\"\n",
    "loaded_model_pca = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71a900b8-db55-4e0d-87ca-dbc59c23321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:57:47 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "[Stage 1644:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|        category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|        pca_features|       rawPrediction|prediction|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                    |            Book|                  []|                  []|       (80870,[],[])|          0.0|        (2000,[],[])|        (2000,[],[])|[0.0,0.0,0.0,0.0,...|[-0.8677515927506...|       0.0|\n",
      "|\"A River Runs Thr...|            Book|[a, river, runs, ...|[river, runs, rem...|(80870,[1,2,4,12,...|          0.0|(2000,[1,2,4,11,1...|(2000,[1,2,4,11,1...|[-3.4263119693083...|[2.41017420359240...|       0.0|\n",
      "|\"Dark City\" is a ...|            Book|[dark, city, is, ...|[dark, city, jour...|(80870,[0,7,8,11,...|          0.0|(2000,[0,7,8,10,1...|(2000,[0,7,8,10,1...|[-4.5290361138155...|[0.55842857409084...|       0.0|\n",
      "|\"Ethical Leadersh...|            Book|[ethical, leaders...|[ethical, leaders...|(80870,[15,29,118...|          0.0|(2000,[14,26,90,2...|(2000,[14,26,90,2...|[-0.2291460713088...|[1.02295663690681...|       0.0|\n",
      "|\"First Comes Marr...|            Book|[first, comes, ma...|[first, comes, ma...|(80870,[1,8,15,17...|          0.0|(2000,[1,8,14,23,...|(2000,[1,8,14,23,...|[-1.0334604258944...|[0.42893063534524...|       0.0|\n",
      "|\"Flyte\" is the se...|            Book|[flyte, is, the, ...|[flyte, second, b...|(80870,[0,3,5,33,...|          0.0|(2000,[0,3,5,30,4...|(2000,[0,3,5,30,4...|[-2.8612623596181...|[2.84240985058397...|       0.0|\n",
      "|\"IN BULGARIA, IN ...|            Book|[in, bulgaria, in...|[bulgaria, muddy,...|(80870,[5,9,12,20...|          0.0|(2000,[5,11,17,30...|(2000,[5,11,17,30...|[-1.0739806313042...|[0.82402051101053...|       0.0|\n",
      "|\"Il Giardino dei ...|            Book|[il, giardino, de...|[il, giardino, de...|(80870,[0,5,7,10,...|          0.0|(2000,[0,5,7,9,10...|(2000,[0,5,7,9,10...|[-9.5123705310923...|[3.23825803382955...|       0.0|\n",
      "|\"In the Woods\" ca...|            Book|[in, the, woods, ...|[woods, summarize...|(80870,[0,1,5,8,9...|          0.0|(2000,[0,1,5,8,10...|(2000,[0,1,5,8,10...|[-2.9428523054190...|[1.85800213260638...|       0.0|\n",
      "|\"Mainly, we retur...|            Book|[mainly, we, retu...|[mainly, return, ...|(80870,[0,1,3,8,9...|          0.0|(2000,[0,1,3,8,10...|(2000,[0,1,3,8,10...|[-8.4493446479999...|[2.67407987701207...|       0.0|\n",
      "|\"Marxist\" is a wo...|            Book|[marxist, is, a, ...|[marxist, word, o...|(80870,[0,1,3,6,1...|          0.0|(2000,[0,1,3,6,9,...|(2000,[0,1,3,6,9,...|[-4.8609313728077...|[1.92159863990561...|       0.0|\n",
      "|\"Minecraft Skin S...|Apps_for_Android|[minecraft, skin,...|[minecraft, skin,...|(80870,[2,13,14,2...|         10.0|(2000,[2,12,13,25...|(2000,[2,12,13,25...|[-0.4531087781367...|[-1.4945009615434...|      10.0|\n",
      "|\"Never forget\" is...|            Book|[never, forget, i...|[never, forget, c...|(80870,[0,1,3,4,5...|          0.0|(2000,[0,1,3,4,5,...|(2000,[0,1,3,4,5,...|[-4.2940230508741...|[1.94416921425531...|       0.0|\n",
      "|\"Presentation Zen...|            Book|[presentation, ze...|[presentation, ze...|(80870,[0,4,14,64...|          0.0|(2000,[0,4,13,57,...|(2000,[0,4,13,57,...|[-1.6761320370276...|[1.14376838464276...|       0.0|\n",
      "|\"Sex and the Gend...|            Book|[sex, and, the, g...|[sex, gender, rev...|(80870,[0,6,13,15...|          0.0|(2000,[0,6,12,14,...|(2000,[0,6,12,14,...|[-4.8610783637641...|[2.26177689570366...|       0.0|\n",
      "|\"Skin Deep\" by Ka...|            Book|[skin, deep, by, ...|[skin, deep, kare...|(80870,[5,8,15,18...|          0.0|(2000,[5,8,14,25,...|(2000,[5,8,14,25,...|[-0.8929122251700...|[1.92652643308934...|       0.0|\n",
      "|\"Teach Yourself S...|            Book|[teach, yourself,...|[teach, spanish, ...|(80870,[0,2,5,9,1...|          0.0|(2000,[0,2,5,12,2...|(2000,[0,2,5,12,2...|[-5.3446066661485...|[3.88508071324166...|       0.0|\n",
      "|\"The Glass Teat\" ...|            Book|[the, glass, teat...|[glass, teat, sta...|(80870,[0,3,5,12,...|          0.0|(2000,[0,3,5,11,1...|(2000,[0,3,5,11,1...|[-2.5472791760486...|[0.05240092307896...|       3.0|\n",
      "|\"The Java EE 5 Tu...|            Book|[the, java, ee, t...|[java, ee, tutori...|(80870,[0,3,4,5,8...|          0.0|(2000,[0,3,4,5,8,...|(2000,[0,3,4,5,8,...|[-6.3342157461529...|[3.57898912269196...|       0.0|\n",
      "|\"The attacks were...|            Book|[the, attacks, we...|[attacks, establi...|(80870,[1,2,3,8,1...|          0.0|(2000,[1,2,3,8,9,...|(2000,[1,2,3,8,9,...|[-3.6678014291052...|[2.08644127858165...|       0.0|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loaded_model_pca.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0830113d-1833-459c-b601-1f44013d3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:58:25 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "[Stage 1645:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|        pca_features|       rawPrediction|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|\"Canada\" is the f...|    Book|[canada, is, the,...|[canada, first, r...|(80870,[0,5,12,15...|          0.0|(2000,[0,5,11,14,...|(2000,[0,5,11,14,...|[-2.6607313311248...|[2.67063565264003...|       0.0|\n",
      "|\"Flee sexual immo...|    Book|[flee, sexual, im...|[flee, sexual, im...|(80870,[0,1,3,12,...|          0.0|(2000,[0,1,3,11,1...|(2000,[0,1,3,11,1...|[-5.0690469242637...|[1.48949373575930...|       0.0|\n",
      "|\"Grant Comes East...|    Book|[grant, comes, ea...|[grant, comes, ea...|(80870,[0,5,29,30...|          0.0|(2000,[0,5,26,27,...|(2000,[0,5,26,27,...|[-1.8174940056556...|[2.40986430069771...|       0.0|\n",
      "|\"How Few Remain\" ...|    Book|[how, few, remain...|[remain, began, t...|(80870,[0,4,12,40...|          0.0|(2000,[0,4,11,36,...|(2000,[0,4,11,36,...|[-1.2348979945981...|[2.26569694772161...|       0.0|\n",
      "|\"It\" is not Steph...|    Book|[it, is, not, ste...|[stephen, king, m...|(80870,[0,1,4,5,9...|          0.0|(2000,[0,1,4,5,10...|(2000,[0,1,4,5,10...|[-6.2175793100796...|[2.63730838007342...|       0.0|\n",
      "|\"King John was no...|    Book|[king, john, was,...|[king, john, good...|(80870,[0,1,2,3,4...|          0.0|(2000,[0,1,2,3,4,...|(2000,[0,1,2,3,4,...|[-7.3574700447478...|[2.67770035979377...|       0.0|\n",
      "|\"Lyra and her dae...|    Book|[lyra, and, her, ...|[lyra, daemon, mo...|(80870,[0,1,2,5,1...|          0.0|(2000,[0,1,2,5,14...|(2000,[0,1,2,5,14...|[-4.3005692381679...|[2.59585716540263...|       0.0|\n",
      "|\"Never Cry Wolf\" ...|    Book|[never, cry, wolf...|[never, cry, wolf...|(80870,[0,6,14,15...|          0.0|(2000,[0,6,13,14,...|(2000,[0,6,13,14,...|[-2.5088771185083...|[0.93322872847400...|       0.0|\n",
      "|\"Perfect Reader\" ...|    Book|[perfect, reader,...|[perfect, reader,...|(80870,[0,3,10,11...|          0.0|(2000,[0,3,9,10,1...|(2000,[0,3,9,10,1...|[-2.4991609103589...|[1.59214898898925...|       0.0|\n",
      "|\"Roll Of Thunder ...|    Book|[roll, of, thunde...|[roll, thunder, h...|(80870,[0,3,4,6,9...|          0.0|(2000,[0,3,4,6,10...|(2000,[0,3,4,6,10...|[-4.3599483780023...|[1.46215580923326...|       0.0|\n",
      "|\"Screwtape Letter...|    Book|[screwtape, lette...|[screwtape, lette...|(80870,[0,4,5,6,2...|          0.0|(2000,[0,4,5,6,18...|(2000,[0,4,5,6,18...|[-2.8267790858451...|[3.18739425632249...|       0.0|\n",
      "|\"Teaching as Lead...|    Book|[teaching, as, le...|[teaching, leader...|(80870,[0,1,2,3,4...|          0.0|(2000,[0,1,2,3,4,...|(2000,[0,1,2,3,4,...|[-11.572994319513...|[2.95579554572319...|       0.0|\n",
      "|\"The Book for Guy...|    Book|[the, book, for, ...|[book, guys, want...|(80870,[0,1,5,9,1...|          0.0|(2000,[0,1,5,15,3...|(2000,[0,1,5,15,3...|[-3.5281112392954...|[1.86801739113572...|       0.0|\n",
      "|\"The Crimson Room...|    Book|[the, crimson, ro...|[crimson, rooms, ...|(80870,[0,7,8,13,...|          0.0|(2000,[0,7,8,12,2...|(2000,[0,7,8,12,2...|[-4.4973949034240...|[2.21916395765401...|       0.0|\n",
      "|\"The Housekeeper ...|    Book|[the, housekeeper...|[housekeeper, pro...|(80870,[0,1,5,7,1...|          0.0|(2000,[0,1,5,7,11...|(2000,[0,1,5,7,11...|[-4.9280496789832...|[1.49133024229934...|       0.0|\n",
      "|\"The Perfect Corp...|    Book|[the, perfect, co...|[perfect, corpora...|(80870,[0,1,3,4,9...|          0.0|(2000,[0,1,3,4,15...|(2000,[0,1,3,4,15...|[-2.8489852995416...|[0.58970992829615...|       0.0|\n",
      "|\"The Shoemaker's ...|    Book|[the, shoemaker, ...|[shoemaker, wife,...|(80870,[0,2,3,5,8...|          0.0|(2000,[0,2,3,5,8,...|(2000,[0,2,3,5,8,...|[-5.6565775431287...|[1.16487229872812...|       0.0|\n",
      "|#1 Detective Davi...|    Book|[detective, david...|[detective, david...|(80870,[0,4,5,8,1...|          0.0|(2000,[0,4,5,8,10...|(2000,[0,4,5,8,10...|[-4.7884610095826...|[2.12933802782006...|       0.0|\n",
      "|&#34;Astonish Me&...|    Book|[astonish, me, by...|[astonish, maggie...|(80870,[2,5,12,13...|          0.0|(2000,[2,5,11,12,...|(2000,[2,5,11,12,...|[-1.2808733349923...|[1.03783620408397...|       0.0|\n",
      "|&#34;Fall of Gian...|    Book|[fall, of, giants...|[fall, giants, fi...|(80870,[0,4,5,11,...|          0.0|(2000,[0,4,5,10,1...|(2000,[0,4,5,10,1...|[-4.2618173517543...|[3.49152186465285...|       0.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pca_predictions = loaded_model_pca.transform(test_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740edb4-473f-46a7-99c9-d5b527378230",
   "metadata": {},
   "source": [
    "## Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality\n",
    "\n",
    "By fitting the training data in the into the pipeline model using pca (dimention reduction method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a92304b8-19cb-4ec2-9d3a-72e01d97965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'one',\n",
       " 'great',\n",
       " 'like',\n",
       " 'good',\n",
       " 'read',\n",
       " 'well',\n",
       " 'love',\n",
       " 'time',\n",
       " 'really',\n",
       " 'much',\n",
       " 'story',\n",
       " 'also',\n",
       " 'use',\n",
       " 'first',\n",
       " 'even',\n",
       " 'product',\n",
       " 'way',\n",
       " 'work',\n",
       " 'new']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_selector_model = loaded_model_pca.stages[4]\n",
    "selected_features = chi_selector_model.selectedFeatures\n",
    "count_vectorizer_model = loaded_model_pca.stages[2]\n",
    "\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "top_terms_pca = [vocabulary[index] for index in selected_features]\n",
    "top_terms_pca[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "721c82df-125d-47d6-8a47-6b6457e89cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_pca_model.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(top_terms_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc5e8434-5f4f-40b8-8db9-9b8edac815b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words common in both files: 1560\n",
      "Number of words unique to file1: 440\n",
      "Number of words unique to file2: 440\n",
      "unique words in output_ds: {'thicker', 'format', 'finish', 'place', 'sized', 'deeper', 'defective', 'personalities', 'relief', 'dreams', 'waiting', 'explanations', 'alpha', 'amusing', 'wild', 'discovery', 'lighter', 'wooden', 'dust', 'day', 'curious', 'meeting', 'doubt', 'cartoon', 'beautifully', 'paranormal', 'useless', 'escape', 'killer', 'thing', 'de', 'excitement', 'editing', 'perfection', 'lift', 'politics', 'addicted', 'bold', 'shared', 'workouts', 'formula', 'blow', 'minds', 'city', 'job', 'lover', 'finally', 'classy', 'finding', 'ancient', 'media', 'facebook', 'surface', 'illustrated', 'helping', 'brain', 'joy', 'uncomfortable', 'explaining', 'foam', 'xbox', 'draws', 'secondary', 'beginner', 'helped', 'pretty', 'arm', 'driven', 'rocks', 'included', 'bar', 'blue', 'engaged', 'check', 'modem', 'wax', 'crisp', 'yoga', 'likes', 'colorful', 'money', 'roku', 'importance', 'errors', 'focuses', 'transfer', 'husband', 'bruce', 'board', 'account', 'team', 'intuitive', 'opinions', 'manufacturer', 'rechargeable', 'reliable', 'driving', 'related', 'includes', 'halloween', 'courage', 'decides', 'williams', 'inside', 'grammar', 'basics', 'convenient', 'business', 'garmin', 'path', 'priced', 'king', 'timeless', 'internal', 'tragedy', 'social', 'silver', 'zip', 'sync', 'ground', 'mild', 'temp', 'rich', 'manual', 'caring', 'super', 'regular', 'wondering', 'boy', 'believes', 'complained', 'assembled', 'sleeping', 'places', 'terms', 'hooked', 'weigh', 'anime', 'home', 'dialog', 'itunes', 'matte', 'eagerly', 'function', 'matter', 'religion', 'legend', 'hate', 'begin', 'critics', 'admit', 'researched', 'adult', 'relaxing', 'sun', 'involving', 'tiny', 'stretch', 'intelligent', 'covers', 'guest', 'fish', 'sequence', 'return', 'info', 'dual', 'rushed', 'overcome', 'medical', 'battles', 'wanna', 'behavior', 'speaks', 'steamy', 'strong', 'male', 'ebook', 'shot', 'elastic', 'titled', 'introduces', 'slippery', 'challenges', 'intense', 'mistakes', 'pics', 'copies', 'learns', 'pick', 'honey', 'removing', 'stations', 'hook', 'neighbors', 'walk', 'worthy', 'faced', 'suggestions', 'looked', 'elegant', 'epic', 'mess', 'wine', 'wide', 'reveals', 'cry', 'peace', 'mixed', 'remarkable', 'principles', 'ease', 'desire', 'lining', 'surprises', 'effective', 'meat', 'eggs', 'opportunity', 'cushion', 'pay', 'tile', 'elements', 'corners', 'roots', 'moves', 'personality', 'practice', 'raw', 'breeze', 'captures', 'message', 'master', 'supporting', 'girls', 'portrayal', 'legendary', 'pain', 'focus', 'width', 'potential', 'dream', 'menu', 'teaches', 'helps', 'cardboard', 'barrel', 'thinking', 'direction', 'analysis', 'electronics', 'spoilers', 'backpack', 'charming', 'stitching', 'battle', 'rides', 'complaints', 'yr', 'idea', 'pictured', 'supplied', 'holding', 'stable', 'control', 'poignant', 'durability', 'building', 'laundry', 'older', 'concept', 'safe', 'dock', 'grasp', 'happening', 'realizes', 'calm', 'rod', 'experience', 'cans', 'subjects', 'realize', 'billy', 'keys', 'empty', 'doctor', 'thickness', 'talks', 'humanity', 'softer', 'sight', 'capability', 'tab', 'circumstances', 'mask', 'families', 'sleeves', 'os', 'mid', 'reception', 'favourite', 'detective', 'peel', 'boost', 'martin', 'fix', 'send', 'grease', 'clips', 'town', 'loose', 'describing', 'include', 'highlight', 'fast', 'shooting', 'controller', 'leads', 'fm', 'shares', 'create', 'recipe', 'clever', 'refund', 'reality', 'snap', 'cloth', 'laughing', 'blender', 'pricey', 'joe', 'public', 'discover', 'sees', 'innovative', 'bose', 'training', 'units', 'touch', 'based', 'worst', 'frame', 'opinion', 'outstanding', 'themes', 'reread', 'ages', 'interpretation', 'leg', 'cried', 'bits', 'mail', 'hand', 'hassle', 'players', 'hardcore', 'moving', 'shame', 'grounds', 'spirit', 'agree', 'efforts', 'recent', 'length', 'attractive', 'lost', 'die', 'sentence', 'overview', 'compared', 'quick', 'act', 'gain', 'angle', 'erotic', 'access', 'publishing', 'stones', 'ft', 'number', 'photos', 'error', 'animals', 'peter', 'order', 'teach', 'tips', 'shelf', 'bucks', 'forced', 'lies', 'revenge', 'obvious', 'downloading', 'coming', 'flat', 'sleep', 'mini', 'notch', 'editor', 'imagination', 'noticeable', 'shots', 'designed', 'west', 'equipment', 'atmosphere', 'plots', 'tighten', 'robert', 'lovers', 'serving', 'growing', 'lord', 'richard', 'steve', 'cheaply', 'subtle', 'prime', 'knowing', 'quickly', 'resolution', 'dr', 'entertained', 'villain', 'sections', 'scratch', 'switched', 'dan', 'securely', 'double', 'blood', 'tears', 'mediocre', 'talking', 'period', 'understood', 'award', 'seconds', 'surround', 'golf'}\n",
      "unique words in output_pca_model: {'scoop', 'printers', 'come', 'oily', 'distortion', 'whose', 'like', 'legos', 'signature', 'normally', 'curl', 'sweetness', 'appetite', 'sponge', 'locks', 'pendant', 'singles', 'droid', 'seed', 'oscar', 'skin', 'rocker', 'cakes', 'using', 'snacks', 'gum', 'allergic', 'looks', 'among', 'creamy', 'lamp', 'shop', 'acne', 'thermostat', 'dough', 'pitcher', 'lag', 'adhesive', 'use', 'rca', 'former', 're', 'help', 'toothbrush', 'take', 'different', 'riff', 'exactly', 'discontinued', 'seats', 'underrated', 'optical', 'ink', 'let', 'becomes', 'mean', 'motherboard', 'laptop', 'cd', 'another', 'flies', 'otter', 'flattering', 'drummer', 'shaving', 'lawn', 'organ', 'grill', 'iphones', 'casting', 'facial', 'hdtv', 'read', 'pin', 'torque', 'steering', 'pedal', 'selections', 'often', 'connectors', 'try', 'fun', 'infant', 'best', 'sneakers', 'kitten', 'soda', 'came', 'yet', 'hair', 'refill', 'nano', 'manually', 'barbie', 'car', 'many', 'brewing', 'becoming', 'damage', 'game', 'allergies', 'bulbs', 'behind', 'strings', 'album', 'ceramic', 'pistol', 'stroller', 'kept', 'together', 'curly', 'case', 'canister', 'keurig', 'also', 'bras', 'mower', 'know', 'wear', 'pepper', 'harmonies', 'fountain', 'given', 'hitch', 'volt', 'tasted', 'remastered', 'leash', 'indie', 'self', 'taste', 'better', 'dog', 'baked', 'salad', 'wrench', 'musically', 'guitar', 'mice', 'actress', 'smartphone', 'flashlights', 'evenly', 'pots', 'product', 'diaper', 'life', 'staple', 'always', 'especially', 'throughout', 'calculator', 'next', 'whole', 'ballads', 'jams', 'smelled', 'perfume', 'n', 'brew', 'mellow', 'x', 'beam', 'wipes', 'riffs', 'shoes', 'conductor', 'newborn', 'flavor', 'patio', 'subtitles', 'monitors', 'disco', 'energetic', 'app', 'sandal', 'certainly', 've', 'nozzle', 'nudity', 'say', 'nokia', 'baby', 'elvis', 'appreciate', 'pasta', 'film', 'ankle', 'pipe', 'soulful', 'see', 'dj', 'performers', 'sonic', 'really', 'tuning', 'shave', 'porch', 'compositions', 'much', 'wonder', 'ensemble', 'cabinets', 'seeing', 'monkey', 'knows', 'truck', 'rented', 'become', 'tanks', 'kitchen', 'buckle', 'samples', 'listeners', 'bake', 'skills', 'slippers', 'sax', 'story', 'staples', 'ethernet', 'printer', 'freezer', 'rifle', 'unplug', 'soles', 'leggings', 'musicianship', 'everyone', 'breast', 'headsets', 'chairs', 'coffee', 'snake', 'used', 'cordless', 'coats', 'broadway', 'lego', 'later', 'credits', 'tried', 'two', 'eaten', 'ar', 'seen', 'xp', 'ever', 'rather', 'liked', 'tasting', 'knife', 'known', 'wrinkles', 'faucet', 'camera', 'rhythms', 'thermometer', 'ounce', 'saw', 'doll', 'soy', 'example', 'want', 'aa', 'seam', 'hairs', 'along', 'mats', 'remix', 'unlock', 'cereal', 'leaking', 'windshield', 'songs', 'mag', 'vehicles', 'vitamins', 'espresso', 'away', 'cinema', 'boiling', 'techno', 'tires', 'phone', 'install', 'symphony', 'laptops', 'toward', 'widescreen', 'spice', 'even', 'r', 'washer', 'producers', 'cookware', 'sit', 'funniest', 'traps', 'way', 'look', 'theatrical', 'standout', 'heartfelt', 'lids', 'must', 'outfits', 'books', 'spout', 'mixer', 'percussion', 'adaptation', 'first', 'toy', 'jars', 'movie', 'think', 'experimental', 'extract', 'done', 'viewers', 'well', 'although', 'songwriting', 'smelling', 'wants', 'chew', 'recieved', 'contains', 'comb', 'tubes', 'cooks', 'vocalist', 'nothing', 'book', 'novel', 'yummy', 'others', 'thoroughly', 'batch', 'particularly', 'ingredient', 'believe', 'rent', 'blackberry', 'provides', 'perhaps', 'comforter', 'almost', 'drip', 'hammer', 'new', 'toshiba', 'bikes', 'bike', 'gallon', 'soil', 'alone', 'ain', 'costumes', 'composed', 'may', 'rust', 'inventive', 'never', 'leaked', 'b', 'drawers', 'need', 'trio', 'mint', 'gig', 'metallica', 'clearly', 'dogs', 'silky', 'rinse', 'truly', 'still', 'curtain', 'chrome', 'gives', 'earphones', 'listens', 'harness', 'bach', 'heats', 'filmed', 'nylon', 'anyone', 'happens', 'rendition', 'clamp', 'performer', 'chewed', 'us', 'course', 'kong', 'subwoofer', 'song', 'knob', 'fusion', 'pillows', 'something', 'follows', 'processor', 'solos', 'old', 'someone', 'moisturizer', 'dries', 'one', 'warmer', 'trunk', 'containers', 'fur', 'crib', 'ginger', 'useful', 'rope', 'beans', 'sang', 'somewhat', 'polished', 'ssd', 'pill', 'every', 'll', 'dolls', 'aroma', 'clasp', 'verizon', 'honda', 'songwriter', 'massage', 'horn', 'almond', 'originals', 'cube', 'wiring'}\n"
     ]
    }
   ],
   "source": [
    "with open('output_ds.txt', 'r') as file1, open('output_pca_model.txt', 'r') as file2:\n",
    "    file1_words = set(file1.read().split())\n",
    "    file2_words = set(file2.read().split())\n",
    "\n",
    "common_words = file1_words & file2_words\n",
    "unique_to_file1 = file1_words - file2_words\n",
    "unique_to_file2 = file2_words - file1_words\n",
    "\n",
    "print(f'Number of words common in both files: {len(common_words)}')\n",
    "print(f'Number of words unique to file1: {len(unique_to_file1)}')\n",
    "print(f'Number of words unique to file2: {len(unique_to_file2)}')\n",
    "\n",
    "print(\"unique words in output_ds:\", unique_to_file1)\n",
    "print(\"unique words in output_pca_model:\", unique_to_file2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b61cd1-4d2b-455a-b706-e90b1f032c3b",
   "metadata": {},
   "source": [
    "# F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38cdc8b6-1b84-4c93-8a51-d5753604f04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_score = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4898727e-10cd-48e5-9a48-da7b0f2d92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6614756080207185\n"
     ]
    }
   ],
   "source": [
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927e7be-5c3f-46dd-9a61-0592b9331318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
