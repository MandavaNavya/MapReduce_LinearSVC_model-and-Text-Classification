{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ad1ab8-bebd-4443-b08f-64fb96b683dd",
   "metadata": {
    "id": "b7ad1ab8-bebd-4443-b08f-64fb96b683dd",
    "outputId": "0627400f-a397-4691-a755-630351a63568",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/26 15:21:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "sc = SparkContext(appName=\"ChiSquare\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee3d00-ec3b-4d4f-9995-8baaf74c9c31",
   "metadata": {
    "id": "f1ee3d00-ec3b-4d4f-9995-8baaf74c9c31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    STOPWORDS = set(line.strip() for line in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e706c16-a24d-4210-8ce0-30b9ede72577",
   "metadata": {
    "id": "9e706c16-a24d-4210-8ce0-30b9ede72577"
   },
   "source": [
    "# Part 1) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef144a2-36a1-4fe5-be61-cdade66424fc",
   "metadata": {
    "id": "eef144a2-36a1-4fe5-be61-cdade66424fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_category(line):\n",
    "    \"\"\"\n",
    "    The function parses the input line and extracts the category of the review.\n",
    "    Input:\n",
    "        line: json representing the review\n",
    "    Output:\n",
    "        category\n",
    "    \"\"\"\n",
    "    review = json.loads(line)\n",
    "    return review[\"category\"]\n",
    "\n",
    "def map_words(line, stopwords):\n",
    "    \"\"\"\n",
    "    The function parses the input line and extracts the category and text from the review.\n",
    "    Then it carries out the tokenization, case folding and stopword filtering on the review text.\n",
    "    Finally, it yields a tuple per every unique word extracted, holding the information regarding the category\n",
    "    Input:\n",
    "        line: line from the text file containing a json representing a review\n",
    "    Output:\n",
    "        key: word\n",
    "        value: (category, 1)\n",
    "    \"\"\"\n",
    "    regex = re.compile(r\"[\\s\\t\\d\\[\\]\\{\\}().!?,;:+=\\-_\\\"'`~#@&*%€$§\\\\/]+\")\n",
    "    review = json.loads(line)\n",
    "    cat = review[\"category\"]\n",
    "    word_list = map(lambda word: word.lower(), regex.split(review[\"reviewText\"]))\n",
    "    unique_words = {word for word in word_list if word and word not in stopwords}\n",
    "    return [(word, (cat, 1)) for word in unique_words]\n",
    "\n",
    "def reducer_words(values):\n",
    "    all_occur = 0\n",
    "    cat_counts = {}\n",
    "    for cat, c in values:\n",
    "        all_occur += c\n",
    "        cat_counts[cat] = cat_counts.get(cat, 0) + c\n",
    "    return [(cat, all_occur, c) for cat, c in cat_counts.items()]\n",
    "\n",
    "def calculate_chi_squared(n, cat_count, all_occur, a):\n",
    "    \"\"\"\n",
    "    Calculates the chi_squared score for category and word based on the provided metrics.\n",
    "    Input:\n",
    "        n: total number of reviews\n",
    "        cat_count: total number of reviews in category\n",
    "        all_occur: total number of occurrences of word\n",
    "        a: number of reviews in given category which contain the given word\n",
    "    Return:\n",
    "        -1 if the calculation encounters division by zero\n",
    "        float chi_squared score otherwise\n",
    "    \"\"\"\n",
    "    c = cat_count - a\n",
    "    b = all_occur - a\n",
    "    d = n - cat_count - b\n",
    "    denom = (a + b) * (a + c) * (b + d) * (c + d)\n",
    "    if denom == 0:\n",
    "        return -1\n",
    "    return n * (a * d - b * c) ** 2 / denom\n",
    "\n",
    "def reducer_scores(values, n, cat_count):\n",
    "    scores = []\n",
    "    for word, all_occur, a in values:\n",
    "        score = calculate_chi_squared(n, cat_count, all_occur, a)\n",
    "        scores.append((score, word))\n",
    "    return sorted(scores, reverse=True)[:75]\n",
    "\n",
    "def run_chi_squared(input_file, output_file):\n",
    "    stopwords_broad = sc.broadcast(STOPWORDS)\n",
    "\n",
    "    reviews = sc.textFile(input_file)\n",
    "\n",
    "    cat_counts = reviews.map(map_category).countByValue()\n",
    "    #cat_counts = dict(reviews.map(map_category).reduceByKey(lambda a, b: a + b).collect())\n",
    "    #print(cat_counts)\n",
    "    n = sum(cat_counts.values())\n",
    "\n",
    "    first_step = reviews.flatMap(lambda x: map_words(x, stopwords_broad.value)).groupByKey().flatMapValues(reducer_words).map(lambda x: (x[1][0], (x[0], x[1][1], x[1][2]))).cache()\n",
    "    second_step = first_step.groupByKey().map(lambda x: (x[0], reducer_scores(x[1], n, cat_counts[x[0]]))).cache()\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for category, scores in sorted(second_step.collect()):\n",
    "            chi_sq = [f\"{word}:{score}\" for score, word in scores]\n",
    "            f.write(f\"<{category}> {' '.join(chi_sq)}\\n\")\n",
    "\n",
    "        all_words = sorted(set(second_step.flatMap(lambda x: x[1]).map(lambda x: x[1]).collect()))\n",
    "        f.write(f\"{' '.join(all_words)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c794c595-5f4a-4fd1-a140-c26d882109b2",
   "metadata": {
    "id": "c794c595-5f4a-4fd1-a140-c26d882109b2",
    "outputId": "83ee6be2-a384-4ec2-a87d-37eddc925a3f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 47.8 ms, total: 227 ms\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_chi_squared(\"reviews_devset.json\",\"output_ChiSquare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019062c-b8e5-464f-a3a5-57a7381e166f",
   "metadata": {
    "id": "6019062c-b8e5-464f-a3a5-57a7381e166f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "run_chi_squared(\"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\",\"output_ChiSquare_fulldata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a123229-7b9f-4e09-93b2-9d689bc19e4c",
   "metadata": {
    "id": "8a123229-7b9f-4e09-93b2-9d689bc19e4c"
   },
   "source": [
    "# Part 2) Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c986f5d-09b1-4fc3-878c-ffd130db734c",
   "metadata": {
    "id": "4c986f5d-09b1-4fc3-878c-ffd130db734c"
   },
   "source": [
    "## DataFrame Creation\n",
    "\n",
    "In this section we create the dataframe from the reviews file, which will contain two columns `reviewText` and `category` extracted from the  review JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2609eeac-076c-41d2-8041-d44884c1f886",
   "metadata": {
    "id": "2609eeac-076c-41d2-8041-d44884c1f886",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE = \"reviews_devset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa45a16-a96c-45ce-8f7a-456bff3a4d50",
   "metadata": {
    "id": "7aa45a16-a96c-45ce-8f7a-456bff3a4d50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TextClassification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5412a1-10ba-4bb4-a3f3-58c6038c4275",
   "metadata": {
    "id": "0c5412a1-10ba-4bb4-a3f3-58c6038c4275",
    "outputId": "7bd33992-82e2-41f6-bce7-4ee128fcf477",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(FILE)\n",
    "df = reviews.select(\"reviewText\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ffab45-f408-4a3a-924e-b888b4c4b963",
   "metadata": {
    "id": "95ffab45-f408-4a3a-924e-b888b4c4b963",
    "outputId": "f2ef7e9a-f781-4426-cdda-5c7d59790171",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d1bcfb-1908-431d-85da-8486675f2c92",
   "metadata": {
    "id": "68d1bcfb-1908-431d-85da-8486675f2c92",
    "outputId": "489e466e-4337-499f-c8aa-4999b7546fec",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\", category='Patio_Lawn_and_Garde')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a87f0e-77e5-444e-b348-c97542e17051",
   "metadata": {
    "id": "b7a87f0e-77e5-444e-b348-c97542e17051"
   },
   "source": [
    "## ML Pipeline\n",
    "\n",
    "In the following section we set up the transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64086786-5785-486e-8cd4-7fecc1e9173e",
   "metadata": {
    "id": "64086786-5785-486e-8cd4-7fecc1e9173e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, ChiSqSelector, StringIndexer, CountVectorizer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28cfede-8f97-41b1-93f1-81699d7ca1b1",
   "metadata": {
    "id": "c28cfede-8f97-41b1-93f1-81699d7ca1b1",
    "outputId": "abb5b931-7cdb-45c5-d483-4207a7cbf1c2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:23:17 WARN DAGScheduler: Broadcasting large task binary with size 1074.0 KiB\n",
      "24/05/26 15:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1076.1 KiB\n",
      "24/05/26 15:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1078.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"allWords\", pattern=r\"[\\s\\t\\d\\[\\]\\{\\}().!?,;:+=\\-_\\\"'`~#@&*%€$§\\\\/]+\") # converts to lowercase and then tokenizes\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=list(STOPWORDS))\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") # category has to be numeric\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=cv.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, indexer, selector])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4fc3bc-3953-482a-b282-e3ff0f288981",
   "metadata": {
    "id": "0c4fc3bc-3953-482a-b282-e3ff0f288981",
    "outputId": "e8cc80a3-5c76-4dab-d914-3a805ed535a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:24:10 WARN DAGScheduler: Broadcasting large task binary with size 1078.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|          reviewText|            category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|This was a gift f...|Patio_Lawn_and_Garde|[this, was, a, gi...|[gift, husband, m...|(96130,[2,3,7,8,3...|         18.0|(2000,[2,3,7,8,35...|\n",
      "|This is a very ni...|Patio_Lawn_and_Garde|[this, is, a, ver...|[nice, spreader, ...|(96130,[0,1,3,21,...|         18.0|(2000,[0,1,3,21,3...|\n",
      "|The metal base wi...|Patio_Lawn_and_Garde|[the, metal, base...|[metal, base, hos...|(96130,[4,10,29,1...|         18.0|(2000,[4,10,174,3...|\n",
      "|For the most part...|Patio_Lawn_and_Garde|[for, the, most, ...|[part, works, pre...|(96130,[1,3,4,9,1...|         18.0|(2000,[1,3,4,9,10...|\n",
      "|This hose is supp...|Patio_Lawn_and_Garde|[this, hose, is, ...|[hose, supposed, ...|(96130,[12,32,42,...|         18.0|(2000,[12,29,101,...|\n",
      "|This tool works v...|Patio_Lawn_and_Garde|[this, tool, work...|[tool, works, cut...|(96130,[0,3,4,8,1...|         18.0|(2000,[0,3,4,8,11...|\n",
      "|This product is a...|Patio_Lawn_and_Garde|[this, product, i...|[typical, usable,...|(96130,[18,63,122...|         18.0|(2000,[18,112,175...|\n",
      "|I was excited to ...|Patio_Lawn_and_Garde|[i, was, excited,...|[excited, ditch, ...|(96130,[6,21,35,3...|         18.0|(2000,[6,21,32,36...|\n",
      "|I purchased the L...|Patio_Lawn_and_Garde|[i, purchased, th...|[purchased, leaf,...|(96130,[3,4,5,6,4...|         18.0|(2000,[3,4,5,6,40...|\n",
      "|Never used a manu...|Patio_Lawn_and_Garde|[never, used, a, ...|[manual, lawnmowe...|(96130,[6,8,41,87...|         18.0|(2000,[6,8,38,78,...|\n",
      "|Good price. Good ...|Patio_Lawn_and_Garde|[good, price, goo...|[good, price, goo...|(96130,[1,13,95,2...|         18.0|(2000,[1,13,226],...|\n",
      "|I have owned the ...|Patio_Lawn_and_Garde|[i, have, owned, ...|[owned, flowtron,...|(96130,[5,17,36,4...|         18.0|(2000,[5,17,33,40...|\n",
      "|I had \"won\" a sim...|Patio_Lawn_and_Garde|[i, had, won, a, ...|[similar, family,...|(96130,[1,11,31,3...|         18.0|(2000,[1,11,28,35...|\n",
      "|The birds ate all...|Patio_Lawn_and_Garde|[the, birds, ate,...|[birds, ate, blue...|(96130,[44,160,28...|         18.0|(2000,[40,144,339...|\n",
      "|Bought last summe...|Patio_Lawn_and_Garde|[bought, last, su...|[bought, summer, ...|(96130,[0,3,7,9,1...|         18.0|(2000,[0,3,7,9,11...|\n",
      "|I knew I had a mo...|Patio_Lawn_and_Garde|[i, knew, i, had,...|[knew, mouse, bas...|(96130,[8,28,29,6...|         18.0|(2000,[8,26,57,80...|\n",
      "|I was a little wo...|Patio_Lawn_and_Garde|[i, was, a, littl...|[worried, reading...|(96130,[1,15,130,...|         18.0|(2000,[1,15,120,1...|\n",
      "|I have used this ...|Patio_Lawn_and_Garde|[i, have, used, t...|[brand, long, tim...|(96130,[2,3,23,25...|         18.0|(2000,[2,3,221,26...|\n",
      "|I actually do not...|Patio_Lawn_and_Garde|[i, actually, do,...|[current, model, ...|(96130,[4,10,16,2...|         18.0|(2000,[4,10,16,20...|\n",
      "|Just what I  expe...|Patio_Lawn_and_Garde|[just, what, i, e...|[expected, works,...|(96130,[0,18,33,4...|         18.0|(2000,[0,18,30,42...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09c9c5-087b-4259-afb3-1a34ee62f165",
   "metadata": {
    "id": "bc09c9c5-087b-4259-afb3-1a34ee62f165"
   },
   "source": [
    "## Top 2000 Terms\n",
    "\n",
    "By fitting the pipeline on all the data we can extract the top 2000 terms according to the chi-squared feature selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8448cc-06f6-484d-a751-71a0bd8e834b",
   "metadata": {
    "id": "2b8448cc-06f6-484d-a751-71a0bd8e834b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_features = model.stages[-1].selectedFeatures\n",
    "vocabulary = model.stages[-3].vocabulary[:(max(top_features) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f9dc81-aaf1-48d0-a5ff-64bb08d2039f",
   "metadata": {
    "id": "b1f9dc81-aaf1-48d0-a5ff-64bb08d2039f"
   },
   "outputs": [],
   "source": [
    "\n",
    "top_terms_model = [vocabulary[i] for i in top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61bf3958-2b63-4bf1-a620-4f63fda5b59d",
   "metadata": {
    "id": "61bf3958-2b63-4bf1-a620-4f63fda5b59d",
    "outputId": "deae36cc-0310-4418-e9df-f0a0ec3367ec",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'good',\n",
       " 'love',\n",
       " 'time',\n",
       " 'work',\n",
       " 'recommend',\n",
       " 'back',\n",
       " 'easy',\n",
       " 'make',\n",
       " 'bought',\n",
       " 'made',\n",
       " 'find',\n",
       " 'buy',\n",
       " 'price',\n",
       " 'put',\n",
       " 'reading',\n",
       " 'quality',\n",
       " 'people',\n",
       " 'works',\n",
       " 'quot']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms_model[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a199f035-e399-407f-a7de-5e18935c5f54",
   "metadata": {
    "id": "a199f035-e399-407f-a7de-5e18935c5f54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(top_terms_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8214bb-2615-4190-aed2-41fbf1ec03af",
   "metadata": {
    "id": "bc8214bb-2615-4190-aed2-41fbf1ec03af",
    "tags": []
   },
   "source": [
    "## Comparison With Assignement 1\n",
    "\n",
    "We performed the chi-square calculation in the first assignement to calculate the top 75 terms in each review category. We now want ot compare these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd52f45-2ce0-4b50-bcd7-f185e23c393e",
   "metadata": {
    "id": "abd52f45-2ce0-4b50-bcd7-f185e23c393e",
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output_1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m top_terms_1 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_1.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      4\u001b[0m     top_terms_1 \u001b[38;5;241m=\u001b[39m lines[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_1.txt'"
     ]
    }
   ],
   "source": [
    "top_terms_1 = []\n",
    "with open(\"output_1.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    top_terms_1 = lines[-1].strip().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64c3e7-704c-469d-99ad-8b3e8179671f",
   "metadata": {
    "id": "9b64c3e7-704c-469d-99ad-8b3e8179671f",
    "outputId": "38969b04-3a16-46f6-8d87-d953585e40dd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_terms_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694b3c4-1ea0-40ff-97ea-342dcaa7ec9d",
   "metadata": {
    "id": "a694b3c4-1ea0-40ff-97ea-342dcaa7ec9d"
   },
   "source": [
    "In the first assignemnt we ended up with 1464 terms in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab194d6-ea03-42ee-90e4-5117b52a44f6",
   "metadata": {
    "id": "eab194d6-ea03-42ee-90e4-5117b52a44f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra = [word for word in top_terms_model if word not in top_terms_1]\n",
    "missing = [word for word in top_terms_1 if word not in top_terms_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fa0e7-b2fe-4d12-a75c-a33367651eb8",
   "metadata": {
    "id": "a51fa0e7-b2fe-4d12-a75c-a33367651eb8",
    "outputId": "f4c7b2ec-af55-44b9-a331-cce8c03eb48c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e14b2c-17be-4266-8b93-acf88e35f695",
   "metadata": {
    "id": "84e14b2c-17be-4266-8b93-acf88e35f695",
    "outputId": "b7021cc0-9a68-428d-b52f-e284e936445d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de291cd-1001-4578-86aa-651e7e92bb4e",
   "metadata": {
    "id": "2afc7781-c75c-4cd4-bb75-bc295e0165b9"
   },
   "source": [
    "Based solely on the number of extra words, and the number of missing words we can conclud that these two results differ greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7608b-ee7e-4dae-a2c9-fa0bdbb1215b",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8fd5c-48b0-4133-b1ec-43257fdad260",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e91e5722-4b4f-493f-9d0b-528e4159cbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:27:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 22:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:27:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, ChiSqSelector, Normalizer\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TextClassification_Pt2\").getOrCreate()\n",
    "\n",
    "# Load the review data\n",
    "df = spark.read.json(\"reviews_devset.json\").select(\"reviewText\", \"category\")\n",
    "df.printSchema()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TextClassification\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584e653-cc49-4a05-960d-565307137e49",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08cc1072-c3e0-4be7-a21b-73e736b2593e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training (70%) and test (30%) sets\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14ef7e-c950-4142-ab83-b85d1ee786a4",
   "metadata": {},
   "source": [
    "## Extend the pipeline and Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd636cc5-da37-4ee6-8e7e-2225b2d79326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the stages of the pipeline\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"allWords\", pattern=r\"[\\s\\t\\d\\[\\]\\{\\}().!?,;:+=\\-_\\\"'`~#@&*%€$§\\\\/]+\") # converts to lowercase and then tokenizes\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\")\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") # category has to be numeric\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=cv.getOutputCol(), outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "normalizer = Normalizer(inputCol=selector.getOutputCol(), outputCol=\"normFeatures\")\n",
    "svc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovr = OneVsRest(classifier=svc, featuresCol=\"normFeatures\", labelCol=\"categoryIndex\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, indexer, selector, normalizer, ovr])\n",
    "\n",
    "# Fit the model\n",
    "# model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c4fe3-ee9e-4b91-a892-59ec2d927f22",
   "metadata": {},
   "source": [
    "### Grid Search for Parameter Optimization¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0cea867-840b-417e-94d7-d022f088c17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1, 0.6]) \\\n",
    "    .addGrid(svc.maxIter, [3, 5]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Set up cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afac5f-e256-4f16-81ba-673b34819024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5d3c8-aa1a-4afc-a12d-4dc862043ba3",
   "metadata": {},
   "source": [
    "### Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2a86e-5c7e-4b69-a716-81b9848d1871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 16:57:43 WARN TaskSetManager: Stage 6832 contains a task of very large size (1487 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model1.bestModel.write().overwrite().save(\"model_devset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e38e5-6475-48ed-bd8b-0cfdfd52e472",
   "metadata": {},
   "source": [
    "### Load the saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c68ab82-07b3-4a8f-9041-dda70b2c7f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_path = \"model_devset\"\n",
    "loaded_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb49ceb-42c4-4d8b-948c-e8605ff03e79",
   "metadata": {},
   "source": [
    "### Transform train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e158fcd2-f35a-43bc-b1d4-e2b063961539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 09:42:01 WARN DAGScheduler: Broadcasting large task binary with size 1973.1 KiB\n",
      "[Stage 134:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|        category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|       rawPrediction|prediction|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|                    |            Book|                  []|                  []|       (80126,[],[])|          0.0|        (2000,[],[])|        (2000,[],[])|[-0.9507807344758...|       0.0|\n",
      "|\"A River Runs Thr...|            Book|[a, river, runs, ...|[river, runs, rem...|(80126,[1,2,4,12,...|          0.0|(2000,[1,2,4,11,1...|(2000,[1,2,4,11,1...|[3.79527585010808...|       0.0|\n",
      "|\"Dark City\" is a ...|            Book|[dark, city, is, ...|[dark, city, jour...|(80126,[0,7,9,11,...|          0.0|(2000,[0,7,8,10,1...|(2000,[0,7,8,10,1...|[1.43746474587411...|       3.0|\n",
      "|\"Ethical Leadersh...|            Book|[ethical, leaders...|[ethical, leaders...|(80126,[15,29,117...|          0.0|(2000,[14,91,262,...|(2000,[14,91,262,...|[2.70737776774149...|       0.0|\n",
      "|\"First Comes Marr...|            Book|[first, comes, ma...|[first, comes, ma...|(80126,[1,9,15,17...|          0.0|(2000,[1,8,14,16,...|(2000,[1,8,14,16,...|[0.98600440287580...|       0.0|\n",
      "|\"Flyte\" is the se...|            Book|[flyte, is, the, ...|[flyte, second, b...|(80126,[0,3,5,35,...|          0.0|(2000,[0,3,5,32,4...|(2000,[0,3,5,32,4...|[3.50869959424166...|       0.0|\n",
      "|\"IN BULGARIA, IN ...|            Book|[in, bulgaria, in...|[bulgaria, muddy,...|(80126,[5,8,12,20...|          0.0|(2000,[5,11,18,32...|(2000,[5,11,18,32...|[1.05267428858325...|       0.0|\n",
      "|\"Il Giardino dei ...|            Book|[il, giardino, de...|[il, giardino, de...|(80126,[0,5,7,10,...|          0.0|(2000,[0,5,7,9,10...|(2000,[0,5,7,9,10...|[4.51065618739562...|       0.0|\n",
      "|\"In the Woods\" ca...|            Book|[in, the, woods, ...|[woods, summarize...|(80126,[0,1,5,8,9...|          0.0|(2000,[0,1,5,8,10...|(2000,[0,1,5,8,10...|[2.94177716481610...|       0.0|\n",
      "|\"Mainly, we retur...|            Book|[mainly, we, retu...|[mainly, return, ...|(80126,[0,1,3,8,9...|          0.0|(2000,[0,1,3,8,10...|(2000,[0,1,3,8,10...|[3.67187047662689...|       0.0|\n",
      "|\"Marxist\" is a wo...|            Book|[marxist, is, a, ...|[marxist, word, o...|(80126,[0,1,3,6,1...|          0.0|(2000,[0,1,3,6,9,...|(2000,[0,1,3,6,9,...|[2.42638773903070...|       0.0|\n",
      "|\"Minecraft Skin S...|Apps_for_Android|[minecraft, skin,...|[minecraft, skin,...|(80126,[2,13,14,2...|         10.0|(2000,[2,12,13,23...|(2000,[2,12,13,23...|[-1.7578103334211...|      10.0|\n",
      "|\"Never forget\" is...|            Book|[never, forget, i...|[never, forget, c...|(80126,[0,1,3,4,5...|          0.0|(2000,[0,1,3,4,5,...|(2000,[0,1,3,4,5,...|[2.82485694413693...|       0.0|\n",
      "|\"Presentation Zen...|            Book|[presentation, ze...|[presentation, ze...|(80126,[0,4,14,62...|          0.0|(2000,[0,4,13,56,...|(2000,[0,4,13,56,...|[2.09058573624004...|       0.0|\n",
      "|\"Sex and the Gend...|            Book|[sex, and, the, g...|[sex, gender, rev...|(80126,[0,6,13,15...|          0.0|(2000,[0,6,12,14,...|(2000,[0,6,12,14,...|[3.51846624136924...|       0.0|\n",
      "|\"Skin Deep\" by Ka...|            Book|[skin, deep, by, ...|[skin, deep, kare...|(80126,[5,9,15,19...|          0.0|(2000,[5,8,14,23,...|(2000,[5,8,14,23,...|[2.71064451787259...|       0.0|\n",
      "|\"Teach Yourself S...|            Book|[teach, yourself,...|[teach, spanish, ...|(80126,[0,2,5,8,1...|          0.0|(2000,[0,2,5,12,1...|(2000,[0,2,5,12,1...|[6.19998789223335...|       0.0|\n",
      "|\"The Glass Teat\" ...|            Book|[the, glass, teat...|[glass, teat, sta...|(80126,[0,3,5,12,...|          0.0|(2000,[0,3,5,11,1...|(2000,[0,3,5,11,1...|[-0.2664276468703...|       0.0|\n",
      "|\"The Java EE 5 Tu...|            Book|[the, java, ee, t...|[java, ee, tutori...|(80126,[0,3,4,5,8...|          0.0|(2000,[0,3,4,5,8,...|(2000,[0,3,4,5,8,...|[5.61731196326532...|       0.0|\n",
      "|\"The attacks were...|            Book|[the, attacks, we...|[attacks, establi...|(80126,[1,2,3,9,1...|          0.0|(2000,[1,2,3,8,9,...|(2000,[1,2,3,8,9,...|[2.13046297032033...|       0.0|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loaded_model.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5053a1f0-bbc5-4930-b039-a5e971b19a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = loaded_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a681f190-3baf-4053-b45d-c147484763ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:32:13 WARN DAGScheduler: Broadcasting large task binary with size 1973.1 KiB\n",
      "[Stage 135:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|       rawPrediction|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|\"Canada\" is the f...|    Book|[canada, is, the,...|[canada, first, r...|(80126,[0,5,12,15...|          0.0|(2000,[0,5,11,14,...|(2000,[0,5,11,14,...|[4.74606906444269...|       0.0|\n",
      "|\"Flee sexual immo...|    Book|[flee, sexual, im...|[flee, sexual, im...|(80126,[0,1,3,12,...|          0.0|(2000,[0,1,3,11,1...|(2000,[0,1,3,11,1...|[2.01272346514844...|       0.0|\n",
      "|\"Grant Comes East...|    Book|[grant, comes, ea...|[grant, comes, ea...|(80126,[0,5,29,30...|          0.0|(2000,[0,5,27,35,...|(2000,[0,5,27,35,...|[4.07803279757908...|       0.0|\n",
      "|\"How Few Remain\" ...|    Book|[how, few, remain...|[remain, began, t...|(80126,[0,4,12,39...|          0.0|(2000,[0,4,11,36,...|(2000,[0,4,11,36,...|[2.82867356728019...|       0.0|\n",
      "|\"It\" is not Steph...|    Book|[it, is, not, ste...|[stephen, king, m...|(80126,[0,1,4,5,8...|          0.0|(2000,[0,1,4,5,10...|(2000,[0,1,4,5,10...|[3.54014960478684...|       0.0|\n",
      "|\"King John was no...|    Book|[king, john, was,...|[king, john, good...|(80126,[0,1,2,3,4...|          0.0|(2000,[0,1,2,3,4,...|(2000,[0,1,2,3,4,...|[4.18239775906921...|       0.0|\n",
      "|\"Lyra and her dae...|    Book|[lyra, and, her, ...|[lyra, daemon, mo...|(80126,[0,1,2,5,1...|          0.0|(2000,[0,1,2,5,14...|(2000,[0,1,2,5,14...|[3.26601969007391...|       0.0|\n",
      "|\"Never Cry Wolf\" ...|    Book|[never, cry, wolf...|[never, cry, wolf...|(80126,[0,6,14,15...|          0.0|(2000,[0,6,13,14,...|(2000,[0,6,13,14,...|[0.72360296039269...|       0.0|\n",
      "|\"Perfect Reader\" ...|    Book|[perfect, reader,...|[perfect, reader,...|(80126,[0,3,10,11...|          0.0|(2000,[0,3,9,10,1...|(2000,[0,3,9,10,1...|[2.31289205894120...|       0.0|\n",
      "|\"Roll Of Thunder ...|    Book|[roll, of, thunde...|[roll, thunder, h...|(80126,[0,3,4,6,8...|          0.0|(2000,[0,3,4,6,10...|(2000,[0,3,4,6,10...|[2.03096146530004...|       0.0|\n",
      "|\"Screwtape Letter...|    Book|[screwtape, lette...|[screwtape, lette...|(80126,[0,4,5,6,2...|          0.0|(2000,[0,4,5,6,19...|(2000,[0,4,5,6,19...|[4.75257201335001...|       0.0|\n",
      "|\"Teaching as Lead...|    Book|[teaching, as, le...|[teaching, leader...|(80126,[0,1,2,3,4...|          0.0|(2000,[0,1,2,3,4,...|(2000,[0,1,2,3,4,...|[4.47410757146904...|       0.0|\n",
      "|\"The Book for Guy...|    Book|[the, book, for, ...|[book, guys, want...|(80126,[0,1,5,8,1...|          0.0|(2000,[0,1,5,15,3...|(2000,[0,1,5,15,3...|[2.48668020295040...|       0.0|\n",
      "|\"The Crimson Room...|    Book|[the, crimson, ro...|[crimson, rooms, ...|(80126,[0,7,9,13,...|          0.0|(2000,[0,7,8,12,1...|(2000,[0,7,8,12,1...|[3.13357408331913...|       0.0|\n",
      "|\"The Housekeeper ...|    Book|[the, housekeeper...|[housekeeper, pro...|(80126,[0,1,5,7,1...|          0.0|(2000,[0,1,5,7,11...|(2000,[0,1,5,7,11...|[2.49992746181813...|       0.0|\n",
      "|\"The Perfect Corp...|    Book|[the, perfect, co...|[perfect, corpora...|(80126,[0,1,3,4,8...|          0.0|(2000,[0,1,3,4,15...|(2000,[0,1,3,4,15...|[0.59829787156894...|       0.0|\n",
      "|\"The Shoemaker's ...|    Book|[the, shoemaker, ...|[shoemaker, wife,...|(80126,[0,2,3,5,8...|          0.0|(2000,[0,2,3,5,8,...|(2000,[0,2,3,5,8,...|[1.71106063192494...|       0.0|\n",
      "|#1 Detective Davi...|    Book|[detective, david...|[detective, david...|(80126,[0,4,5,9,1...|          0.0|(2000,[0,4,5,8,10...|(2000,[0,4,5,8,10...|[3.04476337626524...|       0.0|\n",
      "|&#34;Astonish Me&...|    Book|[astonish, me, by...|[astonish, maggie...|(80126,[2,5,12,13...|          0.0|(2000,[2,5,11,12,...|(2000,[2,5,11,12,...|[1.43294161119960...|       0.0|\n",
      "|&#34;Fall of Gian...|    Book|[fall, of, giants...|[fall, giants, fi...|(80126,[0,4,5,11,...|          0.0|(2000,[0,4,5,10,1...|(2000,[0,4,5,10,1...|[5.22869513481085...|       0.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643e24a-fd65-4d94-9666-76149e7c0162",
   "metadata": {},
   "source": [
    "### Create PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ddd2dd-58f7-4050-9eed-6d18e2b3f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=100, inputCol= selector.getOutputCol(), outputCol=\"pca_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200689d6-4e28-4b94-b82e-f628e78e1496",
   "metadata": {},
   "source": [
    "### Extend the pipeline using pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ecd8220-ab7f-4f9e-8a12-edbb5f7fa7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, indexer, selector, normalizer, pca, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5979c3-f6c7-4264-82a0-76f8a7bc2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:34:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/05/26 15:36:28 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pca_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ea59d-ec81-4a5e-99b8-143719879df7",
   "metadata": {},
   "source": [
    "### Save the pca model and transform train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a614fcf-c1e4-4df8-909c-fcf338587bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:44:11 WARN TaskSetManager: Stage 1304 contains a task of very large size (1504 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/26 15:44:26 WARN TaskSetManager: Stage 1317 contains a task of very large size (1603 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pca_model.write().overwrite().save(\"pca_model_devset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f967ce-cf17-4127-8ae1-b95c0282090d",
   "metadata": {},
   "source": [
    "Load the saved pca model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "febee371-ce23-4b0d-93c8-43820bdc2f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_path = \"pca_model_devset\"\n",
    "loaded_model_pca = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71a900b8-db55-4e0d-87ca-dbc59c23321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:57:47 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "[Stage 1644:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|        category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|        pca_features|       rawPrediction|prediction|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                    |            Book|                  []|                  []|       (80870,[],[])|          0.0|        (2000,[],[])|        (2000,[],[])|[0.0,0.0,0.0,0.0,...|[-0.8677515927506...|       0.0|\n",
      "|\"A River Runs Thr...|            Book|[a, river, runs, ...|[river, runs, rem...|(80870,[1,2,4,12,...|          0.0|(2000,[1,2,4,11,1...|(2000,[1,2,4,11,1...|[-3.4263119693083...|[2.41017420359240...|       0.0|\n",
      "|\"Dark City\" is a ...|            Book|[dark, city, is, ...|[dark, city, jour...|(80870,[0,7,8,11,...|          0.0|(2000,[0,7,8,10,1...|(2000,[0,7,8,10,1...|[-4.5290361138155...|[0.55842857409084...|       0.0|\n",
      "|\"Ethical Leadersh...|            Book|[ethical, leaders...|[ethical, leaders...|(80870,[15,29,118...|          0.0|(2000,[14,26,90,2...|(2000,[14,26,90,2...|[-0.2291460713088...|[1.02295663690681...|       0.0|\n",
      "|\"First Comes Marr...|            Book|[first, comes, ma...|[first, comes, ma...|(80870,[1,8,15,17...|          0.0|(2000,[1,8,14,23,...|(2000,[1,8,14,23,...|[-1.0334604258944...|[0.42893063534524...|       0.0|\n",
      "|\"Flyte\" is the se...|            Book|[flyte, is, the, ...|[flyte, second, b...|(80870,[0,3,5,33,...|          0.0|(2000,[0,3,5,30,4...|(2000,[0,3,5,30,4...|[-2.8612623596181...|[2.84240985058397...|       0.0|\n",
      "|\"IN BULGARIA, IN ...|            Book|[in, bulgaria, in...|[bulgaria, muddy,...|(80870,[5,9,12,20...|          0.0|(2000,[5,11,17,30...|(2000,[5,11,17,30...|[-1.0739806313042...|[0.82402051101053...|       0.0|\n",
      "|\"Il Giardino dei ...|            Book|[il, giardino, de...|[il, giardino, de...|(80870,[0,5,7,10,...|          0.0|(2000,[0,5,7,9,10...|(2000,[0,5,7,9,10...|[-9.5123705310923...|[3.23825803382955...|       0.0|\n",
      "|\"In the Woods\" ca...|            Book|[in, the, woods, ...|[woods, summarize...|(80870,[0,1,5,8,9...|          0.0|(2000,[0,1,5,8,10...|(2000,[0,1,5,8,10...|[-2.9428523054190...|[1.85800213260638...|       0.0|\n",
      "|\"Mainly, we retur...|            Book|[mainly, we, retu...|[mainly, return, ...|(80870,[0,1,3,8,9...|          0.0|(2000,[0,1,3,8,10...|(2000,[0,1,3,8,10...|[-8.4493446479999...|[2.67407987701207...|       0.0|\n",
      "|\"Marxist\" is a wo...|            Book|[marxist, is, a, ...|[marxist, word, o...|(80870,[0,1,3,6,1...|          0.0|(2000,[0,1,3,6,9,...|(2000,[0,1,3,6,9,...|[-4.8609313728077...|[1.92159863990561...|       0.0|\n",
      "|\"Minecraft Skin S...|Apps_for_Android|[minecraft, skin,...|[minecraft, skin,...|(80870,[2,13,14,2...|         10.0|(2000,[2,12,13,25...|(2000,[2,12,13,25...|[-0.4531087781367...|[-1.4945009615434...|      10.0|\n",
      "|\"Never forget\" is...|            Book|[never, forget, i...|[never, forget, c...|(80870,[0,1,3,4,5...|          0.0|(2000,[0,1,3,4,5,...|(2000,[0,1,3,4,5,...|[-4.2940230508741...|[1.94416921425531...|       0.0|\n",
      "|\"Presentation Zen...|            Book|[presentation, ze...|[presentation, ze...|(80870,[0,4,14,64...|          0.0|(2000,[0,4,13,57,...|(2000,[0,4,13,57,...|[-1.6761320370276...|[1.14376838464276...|       0.0|\n",
      "|\"Sex and the Gend...|            Book|[sex, and, the, g...|[sex, gender, rev...|(80870,[0,6,13,15...|          0.0|(2000,[0,6,12,14,...|(2000,[0,6,12,14,...|[-4.8610783637641...|[2.26177689570366...|       0.0|\n",
      "|\"Skin Deep\" by Ka...|            Book|[skin, deep, by, ...|[skin, deep, kare...|(80870,[5,8,15,18...|          0.0|(2000,[5,8,14,25,...|(2000,[5,8,14,25,...|[-0.8929122251700...|[1.92652643308934...|       0.0|\n",
      "|\"Teach Yourself S...|            Book|[teach, yourself,...|[teach, spanish, ...|(80870,[0,2,5,9,1...|          0.0|(2000,[0,2,5,12,2...|(2000,[0,2,5,12,2...|[-5.3446066661485...|[3.88508071324166...|       0.0|\n",
      "|\"The Glass Teat\" ...|            Book|[the, glass, teat...|[glass, teat, sta...|(80870,[0,3,5,12,...|          0.0|(2000,[0,3,5,11,1...|(2000,[0,3,5,11,1...|[-2.5472791760486...|[0.05240092307896...|       3.0|\n",
      "|\"The Java EE 5 Tu...|            Book|[the, java, ee, t...|[java, ee, tutori...|(80870,[0,3,4,5,8...|          0.0|(2000,[0,3,4,5,8,...|(2000,[0,3,4,5,8,...|[-6.3342157461529...|[3.57898912269196...|       0.0|\n",
      "|\"The attacks were...|            Book|[the, attacks, we...|[attacks, establi...|(80870,[1,2,3,8,1...|          0.0|(2000,[1,2,3,8,9,...|(2000,[1,2,3,8,9,...|[-3.6678014291052...|[2.08644127858165...|       0.0|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loaded_model_pca.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0830113d-1833-459c-b601-1f44013d3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:58:25 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "[Stage 1645:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|category|            allWords|               words|         rawFeatures|categoryIndex|    selectedFeatures|        normFeatures|        pca_features|       rawPrediction|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|\"Canada\" is the f...|    Book|[canada, is, the,...|[canada, first, r...|(80870,[0,5,12,15...|          0.0|(2000,[0,5,11,14,...|(2000,[0,5,11,14,...|[-2.6607313311248...|[2.67063565264003...|       0.0|\n",
      "|\"Flee sexual immo...|    Book|[flee, sexual, im...|[flee, sexual, im...|(80870,[0,1,3,12,...|          0.0|(2000,[0,1,3,11,1...|(2000,[0,1,3,11,1...|[-5.0690469242637...|[1.48949373575930...|       0.0|\n",
      "|\"Grant Comes East...|    Book|[grant, comes, ea...|[grant, comes, ea...|(80870,[0,5,29,30...|          0.0|(2000,[0,5,26,27,...|(2000,[0,5,26,27,...|[-1.8174940056556...|[2.40986430069771...|       0.0|\n",
      "|\"How Few Remain\" ...|    Book|[how, few, remain...|[remain, began, t...|(80870,[0,4,12,40...|          0.0|(2000,[0,4,11,36,...|(2000,[0,4,11,36,...|[-1.2348979945981...|[2.26569694772161...|       0.0|\n",
      "|\"It\" is not Steph...|    Book|[it, is, not, ste...|[stephen, king, m...|(80870,[0,1,4,5,9...|          0.0|(2000,[0,1,4,5,10...|(2000,[0,1,4,5,10...|[-6.2175793100796...|[2.63730838007342...|       0.0|\n",
      "|\"King John was no...|    Book|[king, john, was,...|[king, john, good...|(80870,[0,1,2,3,4...|          0.0|(2000,[0,1,2,3,4,...|(2000,[0,1,2,3,4,...|[-7.3574700447478...|[2.67770035979377...|       0.0|\n",
      "|\"Lyra and her dae...|    Book|[lyra, and, her, ...|[lyra, daemon, mo...|(80870,[0,1,2,5,1...|          0.0|(2000,[0,1,2,5,14...|(2000,[0,1,2,5,14...|[-4.3005692381679...|[2.59585716540263...|       0.0|\n",
      "|\"Never Cry Wolf\" ...|    Book|[never, cry, wolf...|[never, cry, wolf...|(80870,[0,6,14,15...|          0.0|(2000,[0,6,13,14,...|(2000,[0,6,13,14,...|[-2.5088771185083...|[0.93322872847400...|       0.0|\n",
      "|\"Perfect Reader\" ...|    Book|[perfect, reader,...|[perfect, reader,...|(80870,[0,3,10,11...|          0.0|(2000,[0,3,9,10,1...|(2000,[0,3,9,10,1...|[-2.4991609103589...|[1.59214898898925...|       0.0|\n",
      "|\"Roll Of Thunder ...|    Book|[roll, of, thunde...|[roll, thunder, h...|(80870,[0,3,4,6,9...|          0.0|(2000,[0,3,4,6,10...|(2000,[0,3,4,6,10...|[-4.3599483780023...|[1.46215580923326...|       0.0|\n",
      "|\"Screwtape Letter...|    Book|[screwtape, lette...|[screwtape, lette...|(80870,[0,4,5,6,2...|          0.0|(2000,[0,4,5,6,18...|(2000,[0,4,5,6,18...|[-2.8267790858451...|[3.18739425632249...|       0.0|\n",
      "|\"Teaching as Lead...|    Book|[teaching, as, le...|[teaching, leader...|(80870,[0,1,2,3,4...|          0.0|(2000,[0,1,2,3,4,...|(2000,[0,1,2,3,4,...|[-11.572994319513...|[2.95579554572319...|       0.0|\n",
      "|\"The Book for Guy...|    Book|[the, book, for, ...|[book, guys, want...|(80870,[0,1,5,9,1...|          0.0|(2000,[0,1,5,15,3...|(2000,[0,1,5,15,3...|[-3.5281112392954...|[1.86801739113572...|       0.0|\n",
      "|\"The Crimson Room...|    Book|[the, crimson, ro...|[crimson, rooms, ...|(80870,[0,7,8,13,...|          0.0|(2000,[0,7,8,12,2...|(2000,[0,7,8,12,2...|[-4.4973949034240...|[2.21916395765401...|       0.0|\n",
      "|\"The Housekeeper ...|    Book|[the, housekeeper...|[housekeeper, pro...|(80870,[0,1,5,7,1...|          0.0|(2000,[0,1,5,7,11...|(2000,[0,1,5,7,11...|[-4.9280496789832...|[1.49133024229934...|       0.0|\n",
      "|\"The Perfect Corp...|    Book|[the, perfect, co...|[perfect, corpora...|(80870,[0,1,3,4,9...|          0.0|(2000,[0,1,3,4,15...|(2000,[0,1,3,4,15...|[-2.8489852995416...|[0.58970992829615...|       0.0|\n",
      "|\"The Shoemaker's ...|    Book|[the, shoemaker, ...|[shoemaker, wife,...|(80870,[0,2,3,5,8...|          0.0|(2000,[0,2,3,5,8,...|(2000,[0,2,3,5,8,...|[-5.6565775431287...|[1.16487229872812...|       0.0|\n",
      "|#1 Detective Davi...|    Book|[detective, david...|[detective, david...|(80870,[0,4,5,8,1...|          0.0|(2000,[0,4,5,8,10...|(2000,[0,4,5,8,10...|[-4.7884610095826...|[2.12933802782006...|       0.0|\n",
      "|&#34;Astonish Me&...|    Book|[astonish, me, by...|[astonish, maggie...|(80870,[2,5,12,13...|          0.0|(2000,[2,5,11,12,...|(2000,[2,5,11,12,...|[-1.2808733349923...|[1.03783620408397...|       0.0|\n",
      "|&#34;Fall of Gian...|    Book|[fall, of, giants...|[fall, giants, fi...|(80870,[0,4,5,11,...|          0.0|(2000,[0,4,5,10,1...|(2000,[0,4,5,10,1...|[-4.2618173517543...|[3.49152186465285...|       0.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pca_predictions = loaded_model_pca.transform(test_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740edb4-473f-46a7-99c9-d5b527378230",
   "metadata": {},
   "source": [
    "## Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality\n",
    "\n",
    "By fitting the training data in the into the pipeline model using pca (dimention reduction method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a92304b8-19cb-4ec2-9d3a-72e01d97965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'one',\n",
       " 'great',\n",
       " 'like',\n",
       " 'good',\n",
       " 'read',\n",
       " 'well',\n",
       " 'love',\n",
       " 'time',\n",
       " 'really',\n",
       " 'much',\n",
       " 'story',\n",
       " 'also',\n",
       " 'use',\n",
       " 'first',\n",
       " 'even',\n",
       " 'product',\n",
       " 'way',\n",
       " 'work',\n",
       " 'new']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `selector` is the Chi-Squared feature selector at position 4\n",
    "chi_selector_model = loaded_model_pca.stages[4]\n",
    "\n",
    "# Get the selected features from the Chi-Squared feature selector\n",
    "selected_features = chi_selector_model.selectedFeatures\n",
    "\n",
    "# Assuming `cv` is the CountVectorizer at position 2\n",
    "count_vectorizer_model = loaded_model_pca.stages[2]\n",
    "\n",
    "# Get the vocabulary from the CountVectorizer model\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "\n",
    "# Get the terms for the selected features\n",
    "top_terms_pca = [vocabulary[index] for index in selected_features]\n",
    "\n",
    "# Display the top terms\n",
    "top_terms_pca[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "721c82df-125d-47d6-8a47-6b6457e89cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_pca_model.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(top_terms_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc5e8434-5f4f-40b8-8db9-9b8edac815b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words common in both files: 1560\n",
      "Number of words unique to file1: 440\n",
      "Number of words unique to file2: 440\n",
      "unique words in output_ds: {'thicker', 'format', 'finish', 'place', 'sized', 'deeper', 'defective', 'personalities', 'relief', 'dreams', 'waiting', 'explanations', 'alpha', 'amusing', 'wild', 'discovery', 'lighter', 'wooden', 'dust', 'day', 'curious', 'meeting', 'doubt', 'cartoon', 'beautifully', 'paranormal', 'useless', 'escape', 'killer', 'thing', 'de', 'excitement', 'editing', 'perfection', 'lift', 'politics', 'addicted', 'bold', 'shared', 'workouts', 'formula', 'blow', 'minds', 'city', 'job', 'lover', 'finally', 'classy', 'finding', 'ancient', 'media', 'facebook', 'surface', 'illustrated', 'helping', 'brain', 'joy', 'uncomfortable', 'explaining', 'foam', 'xbox', 'draws', 'secondary', 'beginner', 'helped', 'pretty', 'arm', 'driven', 'rocks', 'included', 'bar', 'blue', 'engaged', 'check', 'modem', 'wax', 'crisp', 'yoga', 'likes', 'colorful', 'money', 'roku', 'importance', 'errors', 'focuses', 'transfer', 'husband', 'bruce', 'board', 'account', 'team', 'intuitive', 'opinions', 'manufacturer', 'rechargeable', 'reliable', 'driving', 'related', 'includes', 'halloween', 'courage', 'decides', 'williams', 'inside', 'grammar', 'basics', 'convenient', 'business', 'garmin', 'path', 'priced', 'king', 'timeless', 'internal', 'tragedy', 'social', 'silver', 'zip', 'sync', 'ground', 'mild', 'temp', 'rich', 'manual', 'caring', 'super', 'regular', 'wondering', 'boy', 'believes', 'complained', 'assembled', 'sleeping', 'places', 'terms', 'hooked', 'weigh', 'anime', 'home', 'dialog', 'itunes', 'matte', 'eagerly', 'function', 'matter', 'religion', 'legend', 'hate', 'begin', 'critics', 'admit', 'researched', 'adult', 'relaxing', 'sun', 'involving', 'tiny', 'stretch', 'intelligent', 'covers', 'guest', 'fish', 'sequence', 'return', 'info', 'dual', 'rushed', 'overcome', 'medical', 'battles', 'wanna', 'behavior', 'speaks', 'steamy', 'strong', 'male', 'ebook', 'shot', 'elastic', 'titled', 'introduces', 'slippery', 'challenges', 'intense', 'mistakes', 'pics', 'copies', 'learns', 'pick', 'honey', 'removing', 'stations', 'hook', 'neighbors', 'walk', 'worthy', 'faced', 'suggestions', 'looked', 'elegant', 'epic', 'mess', 'wine', 'wide', 'reveals', 'cry', 'peace', 'mixed', 'remarkable', 'principles', 'ease', 'desire', 'lining', 'surprises', 'effective', 'meat', 'eggs', 'opportunity', 'cushion', 'pay', 'tile', 'elements', 'corners', 'roots', 'moves', 'personality', 'practice', 'raw', 'breeze', 'captures', 'message', 'master', 'supporting', 'girls', 'portrayal', 'legendary', 'pain', 'focus', 'width', 'potential', 'dream', 'menu', 'teaches', 'helps', 'cardboard', 'barrel', 'thinking', 'direction', 'analysis', 'electronics', 'spoilers', 'backpack', 'charming', 'stitching', 'battle', 'rides', 'complaints', 'yr', 'idea', 'pictured', 'supplied', 'holding', 'stable', 'control', 'poignant', 'durability', 'building', 'laundry', 'older', 'concept', 'safe', 'dock', 'grasp', 'happening', 'realizes', 'calm', 'rod', 'experience', 'cans', 'subjects', 'realize', 'billy', 'keys', 'empty', 'doctor', 'thickness', 'talks', 'humanity', 'softer', 'sight', 'capability', 'tab', 'circumstances', 'mask', 'families', 'sleeves', 'os', 'mid', 'reception', 'favourite', 'detective', 'peel', 'boost', 'martin', 'fix', 'send', 'grease', 'clips', 'town', 'loose', 'describing', 'include', 'highlight', 'fast', 'shooting', 'controller', 'leads', 'fm', 'shares', 'create', 'recipe', 'clever', 'refund', 'reality', 'snap', 'cloth', 'laughing', 'blender', 'pricey', 'joe', 'public', 'discover', 'sees', 'innovative', 'bose', 'training', 'units', 'touch', 'based', 'worst', 'frame', 'opinion', 'outstanding', 'themes', 'reread', 'ages', 'interpretation', 'leg', 'cried', 'bits', 'mail', 'hand', 'hassle', 'players', 'hardcore', 'moving', 'shame', 'grounds', 'spirit', 'agree', 'efforts', 'recent', 'length', 'attractive', 'lost', 'die', 'sentence', 'overview', 'compared', 'quick', 'act', 'gain', 'angle', 'erotic', 'access', 'publishing', 'stones', 'ft', 'number', 'photos', 'error', 'animals', 'peter', 'order', 'teach', 'tips', 'shelf', 'bucks', 'forced', 'lies', 'revenge', 'obvious', 'downloading', 'coming', 'flat', 'sleep', 'mini', 'notch', 'editor', 'imagination', 'noticeable', 'shots', 'designed', 'west', 'equipment', 'atmosphere', 'plots', 'tighten', 'robert', 'lovers', 'serving', 'growing', 'lord', 'richard', 'steve', 'cheaply', 'subtle', 'prime', 'knowing', 'quickly', 'resolution', 'dr', 'entertained', 'villain', 'sections', 'scratch', 'switched', 'dan', 'securely', 'double', 'blood', 'tears', 'mediocre', 'talking', 'period', 'understood', 'award', 'seconds', 'surround', 'golf'}\n",
      "unique words in output_pca_model: {'scoop', 'printers', 'come', 'oily', 'distortion', 'whose', 'like', 'legos', 'signature', 'normally', 'curl', 'sweetness', 'appetite', 'sponge', 'locks', 'pendant', 'singles', 'droid', 'seed', 'oscar', 'skin', 'rocker', 'cakes', 'using', 'snacks', 'gum', 'allergic', 'looks', 'among', 'creamy', 'lamp', 'shop', 'acne', 'thermostat', 'dough', 'pitcher', 'lag', 'adhesive', 'use', 'rca', 'former', 're', 'help', 'toothbrush', 'take', 'different', 'riff', 'exactly', 'discontinued', 'seats', 'underrated', 'optical', 'ink', 'let', 'becomes', 'mean', 'motherboard', 'laptop', 'cd', 'another', 'flies', 'otter', 'flattering', 'drummer', 'shaving', 'lawn', 'organ', 'grill', 'iphones', 'casting', 'facial', 'hdtv', 'read', 'pin', 'torque', 'steering', 'pedal', 'selections', 'often', 'connectors', 'try', 'fun', 'infant', 'best', 'sneakers', 'kitten', 'soda', 'came', 'yet', 'hair', 'refill', 'nano', 'manually', 'barbie', 'car', 'many', 'brewing', 'becoming', 'damage', 'game', 'allergies', 'bulbs', 'behind', 'strings', 'album', 'ceramic', 'pistol', 'stroller', 'kept', 'together', 'curly', 'case', 'canister', 'keurig', 'also', 'bras', 'mower', 'know', 'wear', 'pepper', 'harmonies', 'fountain', 'given', 'hitch', 'volt', 'tasted', 'remastered', 'leash', 'indie', 'self', 'taste', 'better', 'dog', 'baked', 'salad', 'wrench', 'musically', 'guitar', 'mice', 'actress', 'smartphone', 'flashlights', 'evenly', 'pots', 'product', 'diaper', 'life', 'staple', 'always', 'especially', 'throughout', 'calculator', 'next', 'whole', 'ballads', 'jams', 'smelled', 'perfume', 'n', 'brew', 'mellow', 'x', 'beam', 'wipes', 'riffs', 'shoes', 'conductor', 'newborn', 'flavor', 'patio', 'subtitles', 'monitors', 'disco', 'energetic', 'app', 'sandal', 'certainly', 've', 'nozzle', 'nudity', 'say', 'nokia', 'baby', 'elvis', 'appreciate', 'pasta', 'film', 'ankle', 'pipe', 'soulful', 'see', 'dj', 'performers', 'sonic', 'really', 'tuning', 'shave', 'porch', 'compositions', 'much', 'wonder', 'ensemble', 'cabinets', 'seeing', 'monkey', 'knows', 'truck', 'rented', 'become', 'tanks', 'kitchen', 'buckle', 'samples', 'listeners', 'bake', 'skills', 'slippers', 'sax', 'story', 'staples', 'ethernet', 'printer', 'freezer', 'rifle', 'unplug', 'soles', 'leggings', 'musicianship', 'everyone', 'breast', 'headsets', 'chairs', 'coffee', 'snake', 'used', 'cordless', 'coats', 'broadway', 'lego', 'later', 'credits', 'tried', 'two', 'eaten', 'ar', 'seen', 'xp', 'ever', 'rather', 'liked', 'tasting', 'knife', 'known', 'wrinkles', 'faucet', 'camera', 'rhythms', 'thermometer', 'ounce', 'saw', 'doll', 'soy', 'example', 'want', 'aa', 'seam', 'hairs', 'along', 'mats', 'remix', 'unlock', 'cereal', 'leaking', 'windshield', 'songs', 'mag', 'vehicles', 'vitamins', 'espresso', 'away', 'cinema', 'boiling', 'techno', 'tires', 'phone', 'install', 'symphony', 'laptops', 'toward', 'widescreen', 'spice', 'even', 'r', 'washer', 'producers', 'cookware', 'sit', 'funniest', 'traps', 'way', 'look', 'theatrical', 'standout', 'heartfelt', 'lids', 'must', 'outfits', 'books', 'spout', 'mixer', 'percussion', 'adaptation', 'first', 'toy', 'jars', 'movie', 'think', 'experimental', 'extract', 'done', 'viewers', 'well', 'although', 'songwriting', 'smelling', 'wants', 'chew', 'recieved', 'contains', 'comb', 'tubes', 'cooks', 'vocalist', 'nothing', 'book', 'novel', 'yummy', 'others', 'thoroughly', 'batch', 'particularly', 'ingredient', 'believe', 'rent', 'blackberry', 'provides', 'perhaps', 'comforter', 'almost', 'drip', 'hammer', 'new', 'toshiba', 'bikes', 'bike', 'gallon', 'soil', 'alone', 'ain', 'costumes', 'composed', 'may', 'rust', 'inventive', 'never', 'leaked', 'b', 'drawers', 'need', 'trio', 'mint', 'gig', 'metallica', 'clearly', 'dogs', 'silky', 'rinse', 'truly', 'still', 'curtain', 'chrome', 'gives', 'earphones', 'listens', 'harness', 'bach', 'heats', 'filmed', 'nylon', 'anyone', 'happens', 'rendition', 'clamp', 'performer', 'chewed', 'us', 'course', 'kong', 'subwoofer', 'song', 'knob', 'fusion', 'pillows', 'something', 'follows', 'processor', 'solos', 'old', 'someone', 'moisturizer', 'dries', 'one', 'warmer', 'trunk', 'containers', 'fur', 'crib', 'ginger', 'useful', 'rope', 'beans', 'sang', 'somewhat', 'polished', 'ssd', 'pill', 'every', 'll', 'dolls', 'aroma', 'clasp', 'verizon', 'honda', 'songwriter', 'massage', 'horn', 'almond', 'originals', 'cube', 'wiring'}\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "# Read the contents of both files\n",
    "with open('output_ds.txt', 'r') as file1, open('output_pca_model.txt', 'r') as file2:\n",
    "    file1_words = set(file1.read().split())\n",
    "    file2_words = set(file2.read().split())\n",
    "\n",
    "\n",
    "# Find common and unique words\n",
    "common_words = file1_words & file2_words\n",
    "unique_to_file1 = file1_words - file2_words\n",
    "unique_to_file2 = file2_words - file1_words\n",
    "\n",
    "# Print the results\n",
    "print(f'Number of words common in both files: {len(common_words)}')\n",
    "print(f'Number of words unique to file1: {len(unique_to_file1)}')\n",
    "print(f'Number of words unique to file2: {len(unique_to_file2)}')\n",
    "\n",
    "print(\"unique words in output_ds:\", unique_to_file1)\n",
    "print(\"unique words in output_pca_model:\", unique_to_file2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b61cd1-4d2b-455a-b706-e90b1f032c3b",
   "metadata": {},
   "source": [
    "# F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38cdc8b6-1b84-4c93-8a51-d5753604f04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1528.7 KiB\n",
      "----------------------------------------                            (0 + 8) / 8]\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47452)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o449.evaluate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/ml/evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o449.evaluate"
     ]
    }
   ],
   "source": [
    "f1_score = evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
